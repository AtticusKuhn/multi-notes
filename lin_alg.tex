\documentclass[11pt]{article}
%  \usepackage{evan}
\usepackage{mathtools}
\usepackage{sagetex}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{sectsty}
% \usepackage{graphicx}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\newcommand{\Bold}[1]{#1}
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{Linear Algebra Notes}
\author{Atticus Kuhn}
\date{\today}


\begin{document}
\maketitle
\tableofcontents
\section{March 20}
\begin{defn}
  A \textbf{linear equation} of the variables $x_{1}, x_{2}, \ldots x_{n}$ is any equation of the form
  \[a_{1}x_{1}+a_{2}x_{2} + \cdots + a_{n}x_{n} = b\]
  where $a_{1}, \ldots a_{n}$ are called the coefficents and $b$ is called the constant.
  \end{defn}
  \begin{defn}
    A \textbf{system of linear equation} is a set of 1 or more linear equations.
    \end{defn}
    \begin{ex}
      \[x + 2y = 3\]
      is a linear system and

      \begin{align*}
        x-2y = 1 \\
        2x + 3y = 4 \\
        x + y =  5
        \end{align*}
        is a linear system.
        But
        \[5 \sqrt{x} - y=1\]
        is not a linear system.
      \end{ex}

      Given a linear system, we want to find all substitutions which satisfy the linear
      system.
      A \textbf{solution set} is a set of n-tuples, all of which are valid solutions to a given system of linear equations.
      Our central problem is: given a linear system, find its solution set.
      Big problems
      \begin{enumerate}
        \item Existance Question: Do there exist any solutions? If there are, we call the system \textbf{consistent}. If there are no solutions, we call the solution \textbf{inconsistent}.
              \item Uniqueness Question: How many solutions are there? If there is exactly 1 solution, we call that \textbf{unique}.
      \end{enumerate}
      \begin{thm}
        If a linear system has solutions, it either has 1 solution or infintely many solutions.
        A linear system cannot have 12 solutions.
      \end{thm}
      \begin{ex}
        \[x + 2y = 5, x + 4y = 6\]
        has solution
        \[x = -4, y=9/2\]
        This linear system corresponds to the matrix multiplication
        \[\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\]
        We can turn this into an \textbf{augmented matrix} to get
        \[
          \begin{bmatrix}
            1 & 2 & 5 \\
            3 & 4 & 6
            \end{bmatrix}
        \]
        We are allowed to perform row operations on the rows to get
\[
          \begin{bmatrix}
            1 & 2 & 5 \\
            0 & 1 & 9/2
          \end{bmatrix}
          \implies
          \begin{bmatrix}
            1 & 0 & -4\\
            0 & 1 & 9/2
            \end{bmatrix}
        \]
        So $(x,y) = (-4, 9/2)$. The solution is unique.
      \end{ex}

      \begin{thm}
        We are allowed to do these elementary row operations
        \begin{defn}
          We say two systems of equations are \textbf{equivalent}
          if their solutions sets are equal.
          \end{defn}
        \begin{enumerate}
          \item replacement: replace one row with the sum of itself and another row
          \item interchange: swap 2 rows
                \item scaling: multiply all entries in a  row by a constant
                \end{enumerate}
        \end{thm}
        The reason the elementary operations work is because they
        give an equivalent system, i.e. they do not change the solution set.

        Convince yourself that each of the elementary operations do not change the solution
        set.
        \begin{ex}
          Let's solve this system
          \begin{align*}
            kx + y + 2z &=1 \\
            x + z &= h \\
            y-z &= 1
            \end{align*}
            The solution is
            \[x=-3h/(k-3), y=\frac{(h+1)k-3}{k-3}, z = \frac{hk}{k-3}\]
            The system would have no solutions (inconsistent) iff $k=3$ and $h \neq 0$.
            The system has infinite solutions if $k=3$ and $h= 0$.
            The system has 1 solution if $k \neq 3$.
          \end{ex}
          \begin{defn}
            A matrix is in \textbf{echelon form} iff
            \begin{enumerate}
              \item All nonzero rows are above any rows of all zeroes
              \item each leading entry in a row is in a column to the right of the leading
                    endtry of the row above it
                \item All entries in a column below a leading entry are zeroes
            \end{enumerate}
          \end{defn}
          \begin{defn}
            A matrix is in \textbf{reduce row echelon form} iff
            \begin{enumerate}
              \item The leading entry in each nonzero row is $1$
                    \item each leading $1$ is the only nonzero entry in its column
                    \end{enumerate}
            \end{defn}
            \begin{ex}
              This matrix
              \[
                \begin{bmatrix}
                  * & * & 0 & 0 & 0 & 0 \\
                  0 & * & 1 & 0 & 0 & 0 \\
                  0 & 0 & * & 0 & 0 & *
                  \end{bmatrix}
                \]
                is in REF, but not in RREF.
              \end{ex}
              \begin{ex}
                This matrix
                \[\begin{bmatrix}
                    0 & 1  & 0 & 0 & 0 \\
                    0 & 0 & 2 & 0 & 0 \\
                    0 & 0 & 0 & 0 & 1 \\
                    0 & 0 & 0 & 0 & 0
                    \end{bmatrix}\]
                  is in REF, and it would be in RREF if we changed the $2$ to a $1$.
                \end{ex}

                \begin{thm}
                A given matrix has infinitely many equivalent REF, but only 1 RREF.
                \end{thm}
                \begin{ex}
                  Reduce this matrix
                  \[
                    \begin{bmatrix}
                      1 & -1 & -1 \\
                      0 & 2 & 1 \\
                      2 & 0 & -1
                      \end{bmatrix}
                  \]
                  Solution:
                  \[
\left(\begin{array}{rrr}
1 & 0 & -\frac{1}{2} \\
0 & 1 & \frac{1}{2} \\
0 & 0 & 0
\end{array}\right)
                  \]
   \end{ex}
 \begin{defn}
   If a matrix is in REF, the \textbf{pivots}
   are leading entries. The \textbf{pivot columns} are
   the columns which contain a pivot.
\end{defn}
\begin{defn}
  A \textbf{basic variable} is a variable which corresponds to a pivot column.
  A \textbf{free variable} is a variable which does not correspond to a pivot column.
\end{defn}
\begin{ex}
  Given the matrix
  \[\begin{bmatrix}
      1 & 0 & 2 & 0 & 4 \\
      0 & 1 & 3 & 0 & 5 \\
      0 & 0 & 0 & 1 & 6 \\
    0 & 0 & 0 & 0 & 0 \end{bmatrix}\]
This matrix is in RREF. Variables $x_{1}, x_{2}, x_{4}$ are basic variables
and variable $x_{3}$ is a free variable.
\end{ex}

If the righmost column of $A$ is a pivot column, then we cannot tell if $\mathbf{A}\vec{x} = \vec{b}$ is consistent or inconsistent.
\begin{thm}
  If the rightmost column of
  $\left[\mathbf{A} \quad \vec{b}\right]$ is a pivot column, then
  $\mathbf{A}\vec{x} = \vec{b}$ is not consistent.
\end{thm}

\begin{thm}
  Even if you have a free variable, that doesn't guarantee infintely many solutions.

  If you have a free variable and the system is consistent, then the system has infinitely many solutions.
\end{thm}

\begin{ex}
  Given that the RREF is
  \[\begin{bmatrix}
      1 & 2 & 3 & 0 &  0 &  6 \\
      0 & 0 & 0  & 1 & 0 & 7 \\
      0 & 0 & 0 & 0 & 1 & 8\end{bmatrix}\]
  Describe the solution set.
  It is
  \[
    \begin{bmatrix}
      x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \\ x_{5}
    \end{bmatrix}
    =
    x_{2} \begin{bmatrix} -2  \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
    +
    x_{3} \begin{bmatrix} -3 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}
    +
    \begin{bmatrix} 6 \\ 0 \\ 0 \\ 7 \\ 8 \end{bmatrix}
  \]
  This solution set is infinitely big because it has free varaibles.
  This is called \textbf{parametric vector form} of the solution set.
\end{ex}
\section{March 22}
Let's review what we learned last session about matricies and linear systems.
\begin{enumerate}
  \item the RREF form of a matrix is unique (but the REF form is not).
  \item the elementary row operations do not affect the solution set of a linear system.
  \item pivot columns correspond to basic variables, non-pivot columns correpond to free variables.
  \item every linear system has either 0, 1, or $\infty$ solutions.
  \item an augmented matrix can be used to represent a linear system. It is the coefficent matrix concatenated with the constant vector.
  \item We can express the solution set of a linear system in parametric vector form.
  \item If a system is consistent and has a free variable, it has infinitely many variables (but even inconsistent systems may have free variables)
  \item row operations do not change pivot columns.
\end{enumerate}

\begin{ex}
  Let
  \[p = (0,1) \qquad q = (1,1) \qquad r = (2,2)\]
  Prove there exists no line with all 3 points.

  Prove there exists exactly one unique parabola with all 3 points
  \\
  Solution: \\

  The line has linear system
  \[1  = 0a + b \qquad 1 = a + b \qquad 2 = 2a + b\]
  \begin{align*}
    \begin{bmatrix}
      0 & 1 & 1 \\
      1 & 1 & 1 \\
      2 & 1 & 2 \\
    \end{bmatrix}
    \sim
\left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)
    \end{align*}
\end{ex}
So inconsistent.

For the parabola, it has the system
\[1 = 0a+0b+c \qquad 1 = a + b +c \qquad 2 = 4a + 2b + c\]
\begin{align*}
  \begin{bmatrix}
    0 & 0 & 1 & 1 \\
    1 & 1 & 1 & 1\\
    4 & 2 & 1 & 2 \\
  \end{bmatrix}
  \sim \\
\left(\begin{array}{rrrr}
1 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & -\frac{1}{2} \\
0 & 0 & 1 & 1
\end{array}\right)
\end{align*}
i.e.
\[y=\frac{1}{2}x^{2} - \frac{1}{2}x + 1\]
solution is unique because no free varaibles.
\subsection{1.3 -- Vector Equation}

\begin{defn}
  \textbf{matrix multiplication by vector}: Given
  \[A = [\vec{a_{1}}, \vec{a_{2}}, \vec{a_{3}}, \ldots, \vec{a_{n}}]\]
  and
  \[\vec{v} = \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \\ \vdots \\ v_{n} \end{bmatrix}\]
  then
  \[A\vec{v} = \vec{a_{1}}v_{1}+\vec{a_{2}}v_{2} + \cdots + \vec{a_{n}}v_{n} = \sum_{i=1}^{n} \vec{a_{i}}v_{i}\]
  We call this a \textbf{linear combination} of $a_{i}$. A linear combination is a
  sum of scalings of vectors, i.e. scale the vectors and then add them up.
\end{defn}

\begin{ex}
  \[\begin{bmatrix} 1 & 2 & 3 \\ 5 & 4 & 7 \end{bmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} a + 2b + 3c \\ 5a + 4b + 7c \end{bmatrix}\]
\end{ex}

\begin{defn}
  \textbf{span}: Given a set of vectors $S = \{\vec{v_{1}}, \vec{v_{2}}, \ldots, \vec{v_{n}} \}$, then $span(S)$ is the set of
  all linear combinations of the vectors. In other words, it is the set of all vectors reachable by using only the vectors in $S$.
  In equational form,
  \[span(S) = \left\{ \left .\sum_{i=1}^{n} c_{i} \vec{v}_{i} \;\right |\; \forall c_{i} \in \mathbb{R}\right\}\]
\end{defn}


\begin{ex}
  In $\mathbb{R}^{3}$,
  \[span(\{\vec{v}\}) = \{k\vec{v}  \quad | \quad k \in \mathbb{R}\}\]
  This set is a line going through the origin (unliness $\vec{v} = \vec{0}$, then it's just the origin).
  The $\vec{0}$ vector is always in the span set.

  \[span(\{\vec{u}, \vec{v}\})\]
  this is a plane passing through the origin (unless the vectors are linearly dependent, then it's a line). It could be origin, or line thru
  origin, or plane thru origin.

  \[span(\{\vec{u}, \vec{v}, \vec{w}\})\]
  is either the origin, or a line thru the origin, or a plane thru the origin, or the entire space.
\end{ex}

\begin{ex}
  \[A = \begin{bmatrix} 1  & -1 \\ 2 & 1 \\ 1 & 0 \end{bmatrix}, \qquad v = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}\]
  Is $Ax = b$ consistent?
  This is equivalent to asking if $b$ is a linear combination of the columns of $A$ and it is equivalent to asking
  if $b$ is an element of the spanning set of the columns of $A$.

  This is not consistent because
  \[\left(\begin{array}{rrr}
1 & -1 & 0 \\
2 & 1 & 1 \\
1 & 0 & 2
\end{array}\right)\sim \left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)\]

Thus, $b \not\in span(A)$
\end{ex}

\begin{ex}
  \[x + 2y + 3z = 4 \qquad 5x + 6y + 7z = 8 \]
  So
  \[\begin{bmatrix} 1 & 2 & 3 \\ 5 & 6 & 7 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 4 \\ 8 \end{bmatrix}\]
\end{ex}

\begin{thm}
  Asking whether the system $Ax = b$ is consistent is the same as asking if $b \in span(A)$
\end{thm}
\section{March 23}
\subsection{1.4 -- Matrix Equation}

\begin{ex}
  Let
  \[A =\begin{bmatrix} 1 & 4 & 5 \\ -3 & -1 & -14 \\ 2 & 8 & 10 \end{bmatrix}\]
  Let $b \in \mathbb{R}^{m}$. Is $Ax=b$ consistent for any $b$? \\
  Solution: \\
  Not consistent because if you rref
  \[
\left(\begin{array}{rrrr}
1 & 4 & 5 & b_{1} \\
-3 & -11 & -14 & b_{2} \\
2 & 8 & 10 & b_{3}
\end{array}\right) \]
you get the $-2b_{1} + b_{3} = 0$.

Now, find the set spanned by the column vectors of $A$. \\

(note that the spanning set is NOT $\mathbb{R}^{3}$, because the columns have a
rank of 2).
This is
\[span(A) = \left \{\left . \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \end{bmatrix} \; \right\vert -2b_{1} + b_{3} = 0\right\} = \left\{ b_{2} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} + b_{3} \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} \vert \forall b_{2}, b_{3} \in \mathbb{R}\right\} = span\left(\left\{\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\2\end{bmatrix}\right\}\right)\]
You can do this is Sage with
\begin{sageblock}
 A =  span(column_matrix([[1,4,5],[3,-11,-14], [2,8,10]]))
\end{sageblock}
\end{ex}
\[A = \sage{A}\]
\begin{thm}
  The following statements are logically equivalent:
  Let $A$ be an $m \times n$ matrix and let $b$ be an $m \times 1$ vector.
  \begin{enumerate}
      \item for all $b \in \mathbb{R}^{m}$, $Ax = b$ has a solution.
      \item $span(A) = \mathbb{R}^{m}$
    \item The columns of $A$ span $\mathbb{R}^{m}$
      \item $A$ has 1 pivot position in every row.
      \item $b$ can be written as a linear combination of the columns of $A$.
      \item $Ax = b$ is consistent for all $b \in \mathbb{R}^{m}$.
  \end{enumerate}
\end{thm}

Make sure you don't mess up the details. We want a pivot in every ROW of $A$, not every column of $A$.
\begin{ex}
  Let's apply theorem 7.
  \[span\left(\left\{ \begin{bmatrix} 1 \\ 2 \end{bmatrix}\right\}\right) \neq \mathbb{R}^{2}\]
  because it doesn't have a pivot in the second row.
\end{ex}

\begin{ex}
  \[span\left(\left\{ \begin{bmatrix} 1 \\ 1 \end{bmatrix},\begin{bmatrix} 1 \\ 0 \end{bmatrix},\begin{bmatrix} 2 \\ 1 \end{bmatrix}\right\}\right) = \mathbb{R}^{2}\]
  because there's a pivot in every row.
  Note that there is not a pivot in every column, but the theorem requires a pivot in every row.
\end{ex}

Another mistake people make is thinking that the theorem applies if there is a pivot in every row of $\left [ A \quad b\right ]$. We are only concerned
about a pivot in every row of $A$.
\section{March 27}
\subsection{Homogenous System}
\begin{defn}
  The linear system $Ax=b$ is called \textbf{homogenous} iff $b=\vec{0}$. A homogenous linear system is any system of the form $Ax = \vec{0}$.
\end{defn}
\begin{thm}
  A homogenous linear system is always consistent because \[\mathbf{A}\vec{0} = \vec{0}\]
\end{thm}
\begin{defn}
  We call $x = \vec{0}$ the \textbf{trivial solution} to the linear system $Ax = \vec{0}$.
\end{defn}
\begin{thm}
  A homogenous linear system has infinite nontrivial solutions if and only if it contains free variables.
\end{thm}
\begin{ex}
  consider this homogenous linear system
  \[\begin{bmatrix} 1 & 2 \\ 3  & 6 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\]
  The solution set is
  \[span\left\{\begin{bmatrix} -2 \\ 1 \end{bmatrix}\right\}\]
\end{ex}

\begin{ex}
  consider this homogenous linear system
  \[\begin{bmatrix} 1 & 2 \\ 3  & 6 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 4 \\ 12 \end{bmatrix}\]
  Then
  \[\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} -2y + 4 \\ y \end{bmatrix}\]
  This solution set cannot be written as a span because it doesn't contain $\vec{0}$.
\end{ex}

The solution set of a homogenous system can always be expressed as the span of some vectors.

\begin{ex}
  Here are 2 systems to consider.
  \[S_{1} : x + 2y + 3z = 0\]
  \[S_{2} : x + 2y + 3z = 4 \]
  Compare and contrast the solution sets. \\
  Solution: \\
  The solution set for $S_{1}$ is $span \left\{\begin{bmatrix}-3 \\ 0 \\ 1\end{bmatrix}, \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}\right\}$.
  The solution set for $S_{2}$ is
  \[\left \{\left.\begin{bmatrix} -2y-3z + 4 \\ y \\ z \end{bmatrix} \right| \forall y,z \in \mathbb{R}\right\}\]
\end{ex}

\begin{thm}
  The solution set to $Ax = b$ is parallel to the solution set of $Ax = 0$. You can think of the solution set of $Ax=b$ as being
  a constant vector translation from the solution set of $Ax=0$.

  Suppose $p$ is any solution to the system $\mathbf{A}\vec{x} = \vec{b}$. Then, any other solution to $\mathbf{A}\vec{x} = \vec{b}$ can
  be written as $\vec{p} + \vec{v}_{h}$, where $\vec{v}_{h}$ is any solution to $\mathbf{A}\vec{x} = \vec{0}$.
\end{thm}
Warning: you can only think of the solution set of $\mathbf{A}\vec{x} = \vec{b}$ as a translation of the solution set of $\mathbb{A}\vec{x} = \vec{0}$ is the linear system is consistent. Otherwise, it's solution set is the
empty set.
\subsection{1.6 -- Applications}
There are a bunch of applications of matricies in the textbook.
\begin{ex}
  Find the general traffic pattern in the freeway network shown in the figure \\
  Describe the general traffic pattern when the road $x_{1}$ is closed. \\
  When $x_{4} = 0$ what is the minimum value of $x_{1}$? \\
  Solution:\\
  For the solution, we use the general principle that (flow in) - (flow out) = 0.
  \[A = \begin{bmatrix} 1 & 0 & -1 & -1 & 0 & 40 \\
          -1 & -1 & 0 & 0 & 0 & -200 \\
          0 & 1 & 1 & 0 & -1 & 100 \\
          0 & 0 & 0 & 1 & 1 & 60\end{bmatrix}\]
\begin{align*}
  A &: x_{1} &= x_{3} + x_{4} + 40\\
  B &: 200 &= x_{1} + x_{2} \\
  C &: x_{2} + x_{3} &= 100 + x_{5} \\
  D &: x_{4} + x_{5} &= 60 \\
\end{align*}
This reduces to
\[\left(\begin{array}{rrrrrr}
1 & 0 & -1 & 0 & 1 & 100 \\
0 & 1 & 1 & 0 & -1 & 100 \\
0 & 0 & 0 & 1 & 1 & 60 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right)\]
So $x_{1}, x_{2}, x_{4}$ are basic variables and $x_{3}, x_{5}$ are free variables. The solution set is
\[\left\{\begin{bmatrix} x_{3} - x_{5} + 100 \\ -x_{3} + x_{5} + 100 \\ x_{3} \\-x_{5} + 60 \\ x_{5}\end{bmatrix} | \forall x_{3}, x_{5} \in \mathbb{R}\right\}\]

If $x_{4} = 0$, then $x_{5} = 60$, so we get the solution set

\[\left\{\begin{bmatrix} x_{3} +40 \\ -x_{3} +   160 \\ x_{3} \\ 0  \\ 60\end{bmatrix} | \forall x_{3} \in [0, 160]\right\}\]

The minimum value of $x_{1}$ is $40$.
\subsection{1.7 -- Linear Independence}
\end{ex}
\begin{defn}
  Given a set of vectors $\vec{v}_{1}, \vec{v}_{2}, \vec{v}_{3}, \ldots$, we say that the set is \textbf{linearly independent}
  if and only if the only solution to
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + c_{3}\vec{v}_{3} + \cdots = \vec{0} \qquad c_{i} \in \mathbb{R}\]
  is the trivial solution.

  We say the set is \textbf{linearly dependent} if and only if there is a solution to the equation
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + c_{3}\vec{v}_{3} + \cdots = \vec{0}\qquad c_{i} \in \mathbb{R}\]
  which is not the trivial solution.
\end{defn}

\begin{thm}
  The columns of $\mathbb{A}$ are linearly independent if and only if the only solution to
  \[\mathbf{A}\vec{x} = \vec{0}\]
  is the trivial solution $\vec{x} = \vec{0}$.
\end{thm}
If one vector lies in the plane spanned by the other two, then the vectors are linearly dependent.
\begin{thm}
  To find out if a set of vectors is linearly dependent, RREF their matrix.
\end{thm}
\section{March 29}
No class due to doctor's appointment
\section{March 30}
\subsection{1.7 -- Linearly Indepedence}
\begin{defn}
  A \textbf{linearly independent set} is a set of vectors $\left\{\vec{v}_{1}, \vec{v}_{2}, \ldots \vec{v}_{n}\right\}$ such that
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + \cdots c_{3}\vec{v}_{3} = 0 \iff c_{1} = c_{2} = c_{3} = \cdots = 0 \]
  In other words, we say the columns of $\mathbf{A}$ are linearly independent iff
  \[\mathbf{A}\vec{x} = \vec{0} \iff \vec{x} = \vec{0}\]
\end{defn}

There are some special cases to easily determine if a set is linearly independent or dependent.
\begin{enumerate}
  \item The empty set $\{\}$ is linearly independent.
  \item The singleton set $\{\vec{v}\}$ is linearly independent if and only if $\vec{v} \neq \vec{0}$
  \item The two-element set $\{\vec{v}_{1}, \vec{v}_{2}\}$ is linearly independent iff $\not\exists k \in \mathbb{R}, \vec{v}_{1} = k\vec{v}_{2}$.
  \item Given a set of $p$ vectors $\{\vec{v}_{1}, \vec{v}_{2}, \ldots, \vec{v}_{p}\}$ in $\mathbb{R}^{n}$ and $p > n$, then the set is linearly dependent.
  \item If a set contains the $\vec{0}$ vector, then it is linearly dependent.
\end{enumerate}

Practically, to find out if vectors are linearly independent, RREF the matrix. If the RREF has a pivot in every row, then the vectors are linearly independent. Otherwise, the vectors
are linearly dependent.

\begin{ex}
  Are the column vectors in this matrix linearly independent? Just RREF it.
\end{ex}
\subsection{1.8 -- Linear Transformations}
\begin{defn}
  A \textbf{transformation} is a function $T : \mathbb{R}^{n} \to \mathbb{R}^{m}$. We call $\mathbb{R}^{n}$ the domain
  and we call $\mathbb{R}^{m}$ the codomain.
\end{defn}

\begin{defn}
  Given an $m \times n$ matrix $\mathbf{A}$, a \textbf{Matrix Transformation} is a linear transformation
  \[T: \mathbb{R}^{n} \to \mathbb{R}^{m}\]
  \[T(\vec{x}) = \mathbf{A}\vec{x}\]
  We call $\mathbf{A}$ the \textbf{standard matrix} of $T$.
\end{defn}

\begin{ex}
  For example, given the matrix
  \[\mathbf{A} = \begin{bmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{bmatrix}\]
  The matrix transformation of $\mathbf{A}$ is
  \[T : \mathbb{R}^{2} \to \mathbb{R}^{3}\]
  \[T\left(\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}\right) = \begin{bmatrix} x_{1} - 3x_{2} \\ 3x_{1} + 5x_{2} \\ -x_{1} + 7x_{2} \end{bmatrix}\]
\end{ex}

\begin{defn}
  Given a transformation $T: \mathbb{R}^{n} \to \mathbb{R}^{m}$,
  we say a transformation is a \textbf{linear transformation} iff it satisfies 2 axioms:
  \begin{enumerate}
    \item Vector addition: \[T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v}) \qquad \forall \vec{u}, \vec{v} \in \mathbb{R}^{n} \]
    \item Scaling: \[T(c\vec{v}) = c T(\vec{v}) \qquad \forall c \in \mathbb{R} \quad \forall \vec{v} \in \mathbb{R}^{n}\]
  \end{enumerate}
\end{defn}

\begin{thm}
  If $T: \mathbb{R}^{n} \to \mathbb{R}^{m}$  is a linear transformation, then
  \[T(\vec{0}_{n}) = \vec{0}_{m}\]
  because of the scaling rule.
\end{thm}

By the contrapositive, if $T(\vec{0}) \neq \vec{0}$, then $T$ is not a linear transformation.

\begin{thm}
  All matrix transformations are also linear transformations. Given a linear transformation and a basis, there is a unique matrix transformation corresponding to
  the linear transformation.

  In other words, a matrix transformation is basically the same thing as a linear transformation
\end{thm}

\begin{thm}
  The derivative $\frac{d}{dx}$ from the set of differentiable functions to functions. To see why, it satisfies the axioms
  \[\frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}(f(x)) + \frac{d}{dx}(g(x))\]
  \[\frac{d}{dx}(c f(x)) = c \frac{d}{dx}(f(x))\]
\end{thm}
\section{April 12}

\begin{thm}
  If $ T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ is a linear transformation, then there exists a matrix $A$ called the
  \textbf{standard matrix} defined by
  \[A = [T(e_{1}), T(e_{2}), T(e_{3}), T(e_{4}), \ldots, T(e_{n})]\]
  Where $e_{1}, e_{2}, \ldots, e_{n}$ are the \textbf{basis vectors}.
\end{thm}


The way to see this is that any arbitrary vector $\vec{v} \in \mathbb{R}^{n}$ can be written as a linear combination of the basis vectors as
$\vec{v} = c_{1}\vec{e}_{1} + \cdots c_{n}\vec{e}_{n}$.


\begin{ex}

  The matrix of a  vertical reflection in $\mathbb{R}^{2}$ is
  \[A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\]

  The matrix of a horizontal contraction (or expansion) by a factor of $k$ is
  \[A = \begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}\]


  The matrix of the projection onto the $y$ axis is
  \[A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}\]


  The matrix of the horizonal shear by a factor of $k$ is
  \[A = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}\]


  The matrix of a $\theta$ counter-clockwise about the origin is
  \[A = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} \]
\end{ex}

\begin{defn}
  Let $f : X \to Y$.
  $f$ is called \textbf{surjective} iff every element of $Y$ is hit.

  $f$ is called \textbf{injective} iff $f(x_{1}) = f(x_{2}) \implies x_{1} = x_{2}$.

  Surjective functions are also called ``onto functions''.
  Injective functions are also called ``one-to-one functions''
\end{defn}

\begin{defn}
  We say $f: X \to Y$ is \textbf{bijective} iff it is both surjective and injective.
\end{defn}


\begin{thm}
  Let $T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ be a linear transformation, and let $T$ have standard matrix $A$.

  $T$ is injective/one-to-one iff
  the equation $\textbf{A}\vec{x} = \vec{0}$ only has the solution $\vec{x} = \vec{0}$.
  This means that $A$ has a pivot in every column.


  In other words, $T(\vec{x}) = \vec{0} \implies \vec{x} = 0$.

  The following are equivalent
  \begin{enumerate}
    \item $T$ is an injective/one-to-one function
    \item $A$ has a pivot in every column
    \item $A$ has no free variables
    \item $T(\vec{x}) = \vec{0} \implies \vec{x} = \vec{0}$
    \item The columns of $\mathbf{A}$ are linearly independent.
  \end{enumerate}
\end{thm}


\begin{thm}
  Let $T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ be a linear transformation with matrix $\mathbf{A}$. The follow are equivalent

  \begin{enumerate}
    \item $T$ is a surjection / onto function
    \item $A$ has a pivot in every row
    \item The system $\mathbf{A} \vec{x} = \vec{b}$ is consistent for every $\vec{b} \in \mathbb{R}^{p}$
    \item $span(A) = \mathbb{R}^{p}$.
  \end{enumerate}
\end{thm}


\begin{ex}
  Let $T: \mathbb{R}^{3} \to \mathbb{R}^{2}$ defined by
  \[T(\vec{x}) = \begin{bmatrix} 1  & -1 & 3 \\ -2 & 2 & -6 \end{bmatrix}\vec{x}\]

  Find $Range(T)$? \\
  Solution: \\
  $Range(T) = span(\begin{bmatrix} 1 \\ -2 \end{bmatrix})$\\
  Find the set of all pre-images of $\begin{bmatrix} 2 \\ -4 \end{bmatrix}$?\\
  Solution: \\
  \[S = \left\{ k\begin{bmatrix}1 \\ 1 \\ 0 \end{bmatrix} + l \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix}  + \begin{bmatrix} 2 \\ 0 \\ 0 \end{bmatrix} | \forall k,l \in \mathbb{R}^{3}\right\} = span\left(\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix}\right) + \begin{bmatrix} 2 \\ 0 \\ 0 \end{bmatrix}\]\\
  Is $T$ onto/surjective? \\
  Solution: \\
  no.\\
  Is $T$ one-to-one/injective? \\
  Solution: \\
  no.

\end{ex}
\section{April 13}


\begin{defn}
  Given a matrix $A$, we call $A^{-1}$ the \textbf{inverse} of $A$ iff
  $AA^{-1} = I$
  If $A$ has an inverse, then $A$ is called \textbf{invertable}.

\end{defn}

If $A$ is invertiable, then $A^{-1}$ is its unique inverse.


We only talk about inverses in the context of square matricies.

For a $1 \times 1$ matrix, the inverse is
\[a^{-1} = \frac{1}{a}\]

For a $2 \times 2$ matrix, the inverse is

\[\begin{bmatrix} a & b \\ c & d \end{bmatrix} ^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}\]


\begin{thm}
  If $A$ is invertable, then the system
  \[\mathbf{A}\vec{x} = \vec{b}\]
  has the unique solution
  \[\vec{x} = \mathbf{A}^{-1}\vec{b}\]
\end{thm}

\begin{ex}
  Solve this system
  \[\begin{bmatrix} 1 & 2 \\ 3 & 4\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix} \]

\end{ex}


\begin{thm}
  \[(A^{-1})^{-1} = A\]
  \[(AB)^{-1} = B^{-1}A^{-1}\]
  \[(A^{T})^{-1} = (A^{-1})^{T}\]
\end{thm}

\begin{thm}
  A matrix $A$ is invertible if and only if it is row-equivalent (by elementary row operatins) to
  the identity matrix $I$.
\end{thm}


\begin{thm}
  \textbf{Finding Inverse}: To find the inverse on the $n\times n$ matrix $A$, perform row operations $[A \quad I_{n}]$
  to get to $[I_{n} \quad A]$
  In other words
  \[[A \quad I_{n}] \sim [I_{n} \quad A]\]
\end{thm}

\begin{ex}

  \begin{align*}
    \begin{bmatrix} 2 & 0 & 0 & 1 & 0 & 0 \\ -3 & 0 & 1 & 0 & 1 & 0 \\ 0 & 1  & 0 & 0 & 0 & 1 \end{bmatrix} \\
    &\sim
\left(\begin{array}{rrrrrr}
1 & 0 & 0 & \frac{1}{2} & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & \frac{3}{2} & 1 & 0
\end{array}\right)
  \end{align*}
\end{ex}



\begin{thm}
  \textbf{Invertible Matrix Theorem}:
  The following statements are all logically equivalent
  \begin{enumerate}
    \item $A$ is an invertiable $n \times n$ matrix
    \item $A \sim I_{n}$
    \item $A$ has $n$ pivot positions
    \item The only solution $\mathbf{A}\vec{x} = \vec{0}$ is the trivial solution $\vec{x} = \vec{0}$
    \item The columns of $A$ form a linearly inpendent set/basis of $\mathbb{R}^{n}$
    \item The linear transformation $T(\vec{x}) = \mathbf{A}\vec{x}$ is one-to-one/injective
    \item The linear transformation $T(\vec{x}) = \mathbf{A}\vec{x}$ is onto/surjective
    \item The equation $\mathbf{A}\vec{x} = \vec{b}$ has exactly one solution $\vec{x} = \mathbf{A}^{-1}\vec{b}$ for all $\vec{b}\in \mathbb{R}^{n}$

  \end{enumerate}
\end{thm}
\subsection{Chapter 3 -- Determinants}

\begin{defn}
  Given a matrix $A$, the \textbf{sub-matrix} $A_{ij}$ is the matrix obtained by deleting row $i$ and column $j$
\end{defn}

\begin{thm}
  \textbf{Laplace Expansion}: The determinant can be calculated by
  \[|A| = \sum_{i=1}^{n} (-1)^{i+j} a_{ij}|A_{ij}|= \sum_{j=1}^{n} (-1)^{i+j} a_{ij}|A_{ij}|\]

  Some people call $(-1)^{i+j}|A_{ij}| = c_{ij}$ the \textbf{co-factor}.
\end{thm}

Note that Laplace expansion is very slow, it has time complexity $O(n!)$. If you want to calculate the determinant quickly, elementary row operations can
calculate the determinant in $O(n^{3})$.

\begin{thm}
  If $A$ is a triangular matrix, then $|A|$ is just the product of the diagonal entries.
  \[|A| = \prod_{i=1}^{n}a_{ii}\]
\end{thm}

It is a common mistake that $A \sim B \implies |A| =  |B|$, but this is false.

\begin{thm}
  Each of the elementary row operations has an effect on the determinant\\
  \textbf{scaling}: multiplying a row by $k$ multiplies the determinant by $k$ \\
  \textbf{interchange}: swapping two rows multiplies the determinant by $-1$ \\
  \textbf{replacement}: adding a row to another row doesn't change the determinant \\
\end{thm}

\begin{thm}
  \textbf{Properties of determinant}
  \[|AB|=|A||B|\]
  \[|A^{-1}|=|A|^{-1}\]
  \[|A^{T}|=|A|\]
\end{thm}

\section{April 17}

Let's review determinant.
\begin{align*}
  \begin{vmatrix} d-2g & e-2 & g-2i \\
    3a  & 3b & 3c \\
    -a  + 4g & -b + 4h & -c + 4i \end{vmatrix} \\
  &= -3 \cdot 4 \cdot \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix}
\end{align*}
\begin{thm}
  \textbf{Cramers Rule}: If $A$ is an invertiable $n \times n$ matrix, then the solution to the linear system
  \[\mathbf{A}\vec{x} = \vec{b}\]
  has entries
  \[\vec{x}_{i} = \frac{\det \mathbf{A}_{i}(\vec{b})}{\det \mathbf{A}}\qquad \forall i \in \{1,2,\ldots , n\} \]
  Where
  \[\mathbf{A}_{i}(\vec{b}) := [\vec{a}_{1} \ldots \vec{b} \ldots \vec{a}_{n}]\]
\end{thm}

\begin{thm}
The determinant is the (signed) volume or area of the parallepiped or parallelogram with edges given by the columns of $\mathbf{A}$.
\end{thm}

\subsection{Practice Knowledge Check Chapter 1}
\begin{enumerate}
  \item Consider the following System \[x + y + z = 2 \quad x - y + z = h \quad x - y + kz = 3 \]
        \begin{enumerate}
          \item apply elementary row oeprations to write in echelon form.
                \[\begin{bmatrix} 1 & 1 & 1 & 2 \\ 0 & -2 & 0 & h-2 \\ 0 & 0 & 0 & 3-h \end{bmatrix}\]
          \item Determine the values of $k, h$ such that the system has 0, 1, or $\infty$ solutions?
                $0$ solutions iff $k=1 \land h \neq 3$. $1$ solution iff $k \neq 1$, $\infty$ solutions
                iff $k=1 \land k=3$
          \item When the system has $\infty$ solutions, find the general solution in parametric vector form
                \[\begin{bmatrix} 5/2 \\ -1/2 \\ 0 \end{bmatrix} + span \begin{bmatrix} -1 \\ 0 \\1 \end{bmatrix}\]
          \item describe the above set geometrically?
                A line passing through $\begin{bmatrix} 5/2 \\ -1/2 \\ 0 \end{bmatrix}$ and parallel to $\begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$
                in $\mathbb{R}^{3}$
        \end{enumerate}
         \item Let $\mathbf{A} = \begin{bmatrix} 1 & -1 & -1 \\ 0 & 2 & 1 \\ 2 & - & -1 \end{bmatrix}$
        \begin{enumerate}
          \item Do the columns of $A$ span $\mathbb{R}^{3}$?
                No
          \item Find the span of the columns of $A$?
                \[span \left\{\begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} , \begin{bmatrix} -1  \\ 2 \\ 0 \end{bmatrix}\right\}\]
          \item Are the columns of $A$ linearly independent?
                No
          \item Write a dependence relation among the columns of $A$?
                \[\vec{a}_{3} = -\frac{1}{2}\vec{a}_{1} + \frac{1}{2}\vec{a}_{2}\]
          \item Geometrically describe $span \left\{a_{1}, a_{2}, a_{3}\right\}$?
                A place in $\mathbb{R}^{3}$ passing through the origin $\begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}$, and
                $\begin{bmatrix} -1 \\ 2 \\ 0 \end{bmatrix}$.
        \end{enumerate}
        \item Let $T : \mathbb{R}^{3} \to \mathbb{R}^{2}$ be a linear transformation that maps $(1,1,1) \mapsto (0,1)$, $(0,1,1) \mapsto (1,1)$, and $(0,1,1) \mapsto (1,-1)$
        \begin{enumerate}
          \item Is $T$ a matrix transformation?\\
                Yes
          \item Find the matrix of $T$?\\
                \[T(\vec{x}) = \begin{bmatrix} -1 & 1 & 0 \\ 2 & 1 & -2 \end{bmatrix} \vec{x}\]
          \item Find the image of $(x,y,z)$?\\
                \[\begin{bmatrix} -x+y \\ 2x+y-2z \end{bmatrix}\]
          \item Prove that $T$ is a linear transformation?\\
                \[T(\vec{a} + \vec{b}) = T(\vec{a} + \vec{b})\]
                \[T(c\vec{a}) = cT(\vec{a})\]
          \item Find the pre-image of $(1, 4)$ \\
                \[S = \left\{\left. \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}  + \begin{bmatrix} 2/3 \\ 2/3 \\ 1 \end{bmatrix} z \right | \forall z \in \mathbb{R}\right\}\]
          \item Is $T$ one-to-one/injective?\\
                no\\
          \item Is $T$ onto/surjective? \\
          yes
        \end{enumerate}

\end{enumerate}

\section{April 20}
\begin{defn}
  A set $V$ is called a \textbf{vector space} iff it satisfies the following axioms
  \begin{align*}
    \vec{u} + \vec{v} &= \vec{v} + \vec{u} \\
    (\vec{u} + \vec{v}) + \vec{w} &= \vec{u} + (\vec{v} + \vec{w})\\
    \vec{v} + \vec{0} &= \vec{v} \\
    \vec{u} + (-\vec{u}) &=0 \\
    c(\vec{u} + \vec{v}) &= c\vec{u} + c\vec{v} \\
    (c+d)\vec{u} &= c\vec{u}+d\vec{u}\\
    c(d\vec{u}) &= (cd)\vec{u} \\
    1\vec{u} &= \vec{u}
  \end{align*}
\end{defn}

Note that the empty set is \emph{not} a vector space because it does not contain $\vec{0}$.


The \textbf{zero space} $\{\vec{0}\}$ is a vector space.

\begin{ex}
  Examples of vector spaces
  \begin{enumerate}
    \item the zero space $\{\vec{0}\}$.
    \item $\mathbb{R}^{n}$ for any integer $n$.
    \item $\mathbb{P}$, the set of polynomials with real coefficients
    \item $\mathbb{P}_{n}$, the set of polynomials with real coefficients with degree at most $n$.
    \item signals: $\mathbb{S}$ the set of lists of real numbers that extend infinitely in both directions $(\ldots, x_{-1}, x_{0}, x_{1}, \ldots)$
    \item function space: real valued functions from any set $X$ to the real numbers $\mathbb{R}^{n}$
    \item Any plane passing through the origin.
  \end{enumerate}
\end{ex}

\section{April 24}
Nothing happend.
\section{April 26}
Here are some more examples of vector spaces
\begin{enumerate}
  \item The set of $m \times n$ matricies
  \item The set of integrable functions from a set $X$ to $\mathbb{R}$.
  \item For a fixed $m \times n$ matrix $B$, the set $\left\{ \mathbf{A} | \forall \mathbf{A} \in \mathbb{R}^{p \times m} ,  \mathbf{A}\mathbf{B} = \mathbf{0}\right\}$ (because it is the kernel/nullspace of $\mathbf{B}$).
  \item For a fixed $m \times n$ matrix $B$, the set $\left\{\mathbf{A} | \forall \mathbf{A} \in \mathbb{R}^{p \times m} , \mathbf{AB} = \mathbf{A}\right\}$ (because it is an invariant subspace)
\end{enumerate}

\begin{defn}
  Given a vector space $V$, we say that a set $W$ is called a \textbf{vector subspace} of $V$ if and only if
  \begin{enumerate}
    \item $W \subseteq V$
    \item $\vec{0} \in W$
    \item addition is closed under $W$, meaning that $\forall \vec{u},\vec{v} \in W \implies (\vec{u} + \vec{v}) \in W$
    \item $W$ is closed under scalar multiplication, meaning that $\forall c \in \mathbb{R} \quad \forall \vec{u} \in W \implies k\vec{u} \in W$.
  \end{enumerate}
\end{defn}


\begin{ex}
  Here is a list of examples of vector subspaces
  \begin{enumerate}
    \item For any vector space $V$, the zero subspace $\{\vec{0}\}$ is a subspace of $V$
    \item $\left \{ \begin{bmatrix} a \\ b \\ 0 \end{bmatrix} | \forall a , b \in \mathbb{R} \right\}$ is a vector subspace of $\mathbb{R}^{3}$
    \item $\left\{ \begin{bmatrix} x \\ y \\ x + y \end{bmatrix} | \forall x , y \in \mathbb{R} \right\}$ is a vector subspace of $\mathbb{R}^{3}$
\end{enumerate}
\end{ex}
\begin{thm}
  If $W$ is a vector subspace of $V$, then $W$ is automatically a vector space (you don't need to check the axioms).
\end{thm}

\begin{thm}
  If $V$ is a vector space and $S \subseteq V$, then $span(S)$ is a vector subspace of $V$.
\end{thm}


\begin{defn}
  Given a linear transformation $T : V \to W$, we define the \textbf{null space} as
  \[Nul(T) := \left\{ \vec{x} | \vec{x} \in V , T(\vec{x}) = \vec{0} \right\} = T^{-1}(\vec{0})\]

  It is a theorem that $Nul(T)$ is a vector subspace of $V$. The null space is sometimes called the kernel of $T$.
\end{defn}

\begin{defn}
  Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, we define the \textbf{column space} as
  \[Col(A) := span(A) = \left\{\mathbf{A}\vec{x} | \forall \vec{x} \in \mathbb{R}^{n}\right\}=  \left\{\vec{b} | \quad  \exists \vec{x} \in \mathbb{R}^{m \times n}, \mathbf{A}\vec{x} = \vec{b}\right \}\]
  The column space is also called the \textbf{range} of the linear transformation $\vec{x} \mapsto \mathbf{A} \vec{x}$.

  It is a theorem that $Col(\mathbf{A})$ is a vector subspace of $\mathbb{R}^{m}$.
\end{defn}

\begin{ex}
  Let \[A := \begin{bmatrix} 1 & 1 & 1  \\ 1 & 2 & -1 \\ 2 & 4 & -2 \end{bmatrix}\]
  Find the column space and the null space.
  \[Col(A) = span(\{\begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix} , \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}\})\]
  \[Nul(A) = span(\{\begin{bmatrix} -3 \\ 2 \\ 1 \end{bmatrix}\})\]
\end{ex}
\section{April 27}

\begin{ex}
  Consider $T : \mathbb{P}_{2}  \to \mathbb{R}^{2 \times 3}$
  \[T(p(t) = \begin{bmatrix} p(0) & p(0) & p(1) \\ p(1) & p(1) & p(0) \end{bmatrix}\]

  Prove that $T$ is a linear transformation. \\
  $T$ satisfies the linear transformation axioms. $T$ preserves vector addition and preserves scalar multiplication.

  Find $Ker(T)$.

  \[Ker(T) = span \{ \lambda x. x^{2} - x \}\]

  Is $T$ a matrix transformation? According to YHPL, no.

  Find $Range(T)$.
  \[Range(T) = span \left\{ \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}, \begin{bmatrix} 0 & 0 & 1 \\ 1 & -1 & 0 \end{bmatrix}, \right\}\]

  Find all pre-images of $\begin{bmatrix} 2 & 2 & 1 \\ 1 & 1 & 2 \end{bmatrix}$?
  \[T^{-1}(\begin{bmatrix} 2 & 2 & 1 \\ 1 & 1 & 2 \end{bmatrix}) = \left\{ \lambda x. ax^{2}- (1 + a) x + 2 | \forall a \in \mathbb{R}\right\}\]


  Is $T$ injective/one-to-one? No because nullspace is not trivial.


  Is $T$ surjective/onto? Impossible because the codomain is larger than the domain.

\end{ex}
\subsection{4.3 -- Basis}

\begin{defn}
  Let $V$ be a vector space. We say that $B \subseteq V$ is called a \textbf{basis} of $V$ if and only if
  \begin{enumerate}
    \item $span(B) = V$
    \item $B$ is linearly independent
  \end{enumerate}
  In other words, a basis is a smallest spanning set.
\end{defn}

Note that a basis of $V$ is not unique: $V$ usually has infinitely many different bases.
For example, $\{1,x,x^{2}\}$ is a basis of $\mathbb{P}_{2}$, but $\{1 + x, x^{2}, x + x^{2}\}$ is also a basis of $\mathbb{P}_{2}$.
\begin{ex}
  Here are some examples of a basis
  \begin{enumerate}
    \item $\left\{\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\right\}$ is a basis of $\mathbb{R}^{3}$
    \item $\{1, x, x^{2}\}$ is a basis of $\mathbb{P}_{2}$

  \end{enumerate}
\end{ex}

\begin{thm}
  If $S$ spans $V$, then any superset of $S$ also spans $V$.

  If $S$ is a linearly independent set, then any subset of $S$ is also linearly indepenent.

  If $S$ is a linearly dependent set, then any superset of $S$ is also linearly depenent.

  If $B$ is a basis of $V$, then you cannot add or remove any vector from $B$ and still have it be a basis of $V$.
\end{thm}
\section{May 1}
\begin{ex}
  Does the set $\{1+x, x^{2},x+x^{2}\}$ span $\mathbb{P}_{2}$? Yes.

  To see why, we need to show that an abitrary vector, call it $a + bx + cx^{2}$ can be written as a linear combination of the basis.
  We want to show that $\exists k,l,m \in \mathbb{R}$
  \[k(1+x) + lx^{2} + m(x + x^{2}) = a + bx + cx^{2} \implies a + bx  + cx^{2} = k + (k + m) x + (l + m)x^{2}\]
  So we get the linear system
  \begin{align*}
    l + m &=a \\
    k + m &= b \\
    k &= c
  \end{align*}
  \[\begin{bmatrix} 0 & 1 & 1  & a\\ 1 & 0 & 1 & b \\ 1 & 0 & 0 & c \end{bmatrix} \]
\end{ex}

Be warned that the pivot columns of $A$ form a basis of $Col(A)$, not the pivots in the reduced matrix.

\subsection{4.4 -- Coordinate Systems}

\begin{thm}
  \textbf{Unique representation theorem}: If $B$ is a basis of $V$, then every vector $v \in V$ can be uniquely
  represented as a linear combination of $B$. In other words, there exist unique scalars $c_{1}, c_{2}, \ldots c_{n}$ such that
  \[v = c_{1}\vec{b}_{1}+c_{2}\vec{b}_{2}+\cdots+c_{n}\vec{b}_{n}\]
  There is one and only one set of scalars $c_{1}, \ldots, c_{n}$ to satisfy the equation.
\end{thm}


\begin{defn}
  With respect to a given basis $B$, we define the \textbf{coordinates} relative to $B$ to a notation as
  \[[\vec{x}]_{B}=\begin{bmatrix}c_{1}\\c_{2}\\\vdots\\c_{n}\end{bmatrix}_{B} = c_{1}\vec{b}_{1}+c_{2}\vec{b}_{2}+\cdots+c_{n}\vec{b}_{n}\]
  This notation is well-defined because of the unique representation theorem.
\end{defn}

A vector space may have many different bases.


To change a coordinate vector from $B_{1}$ to $B_{2}$ there exists a matrix called the \textbf{change of basis} matrix which changes the basis.

\begin{ex}
  Let $a = \langle 6,5 \rangle$ in  $\begin{bmatrix} 6 \\ 5 \end{bmatrix}$ is the coordinate vector in the elementary basis. If we set our basis
  to $\{\langle 3,1 \rangle, \langle 0, 1\rangle\}$, then the coordinate vector is $\begin{bmatrix} 2 \\ 3 \end{bmatrix}_{B}$ because
  $\begin{bmatrix} 2 \\ 3 \end{bmatrix}_{B} = 2\langle 3,1 \rangle + 3 \langle 0,1\rangle = \langle 6,5\rangle$

  In this case, the change-of-basis matrix $\begin{bmatrix} 3 & 0 \\ 1 & 1 \end{bmatrix}$.
  \[\begin{bmatrix} 3 & 0 \\ 1 & 1\end{bmatrix}\begin{bmatrix} e_{1} \\ e_{2} \end{bmatrix} =\begin{bmatrix} b_{1}\\b_{2}\end{bmatrix}\]
\end{ex}

\begin{ex}
  Let $v = \mathbb{P}_{2}$ and let $B = \{1+t, t^{2}, t+t^{2}\}$.
  \[[1+2t+3t^{2}]_{B} = \begin{bmatrix} 1 \\2 \\ 1\end{bmatrix}\]
  because
  \[1+2t+3t^{2} = 1(1+t) + 2(t^{2})+1(t+t^{2})\]
\end{ex}
\begin{thm}
  \[\mathbb{P}_{n} \cong \mathbb{R}^{n+1}\]
  Because we haev the bijective linear mapping (called a vector-space isomorphism)
  one example is
  \[T(a +bx + cx^{2}) \mapsto \begin{bmatrix} a \\ b \\ c \end{bmatrix}\]
\end{thm}

\begin{ex}
  Let $B = \left\{ \begin{bmatrix} 3 \\ 3 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix}\right\}$.
  Find the coordinate vector of $\begin{bmatrix} 9 \\ 13 \\ 15\end{bmatrix}$ in $span(B)$.
  \[\begin{bmatrix} 9 \\ 13 \\ 15 \end{bmatrix}_{B} = \begin{bmatrix} 3 \\ 4\end{bmatrix}\]
  Becuase
  \[3\begin{bmatrix} 3 \\ 3 \\ 1 \end{bmatrix} + 4 \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 9 \\ 13 \\ 15 \end{bmatrix}\]
\end{ex}


\begin{defn}
  If $B$ is a basis of $V$, then the \textbf{dimension} of $V$ is the length/cardinality of $B$.

  No matter which basis we choose, each basis of $V$ will have the same length, which is the basis of $V$.
\end{defn}

\begin{ex}
  The vector space $\mathbb{R}^{n}$ is $n$-dimensional because any basis of $\mathbb{R}^{n}$ has $n$ elements.
\end{ex}

\begin{ex}
  The trivial vector space $\{\vec{0}\}$ is $0$-dimensional because the empty list $\{\}$ spans the vector space.
  ${\vec{0}}$ is the only $0$-dimensional vector space.
\end{ex}


\begin{thm}
  If $V$ is a vector space of dimension $p$, then any linearly independent set of size $p$ is automatically a basis of $V$.
\end{thm}

\begin{thm}
  \[dim(\vec{0})=0\]
  \[dim(\mathbb{R}^{n})=n\]
  \[dim(\mathbb{P}_{n})=n+1\]
\end{thm}

\subsection{4.6 -- Rank}
\begin{defn}
  Given a linear transformation $T : V \to W$ we define the \textbf{rank} as
  \[rank(T) := dim(Range(T))\]
\end{defn}

\begin{thm}
  To calculate the rank of a matrix, the rank is the number of pivot columns,
  which is equal to the number of pivot rows.
\end{thm}

\begin{defn}
  Given a linear transformation $T: V \to W$, we define the \textbf{nullity} as
  \[dim(ker(T))\]
\end{defn}


\begin{thm}
  \textbf{rank-nullity theorem}: If $T : V \to W$ is a linear transformation, then the rank and the nullity
  are related by the following equation
  \[dim(range(T)) + dim(ker(T)) = dim(V)\]

  To prove this, the rank is the number of pivot columns, and the nullity is the number of nonpivot columns,
  and $dim(V)$ is the total number of columns.
\end{thm}
\section{May 3}

\begin{defn}
  If $A \in \mathbb{R}^{n \times n}$ is a matrix and $\vec{x} \in \mathbb{R}^{n}$ is a vector with $\vec{x} \neq \vec{0}$ and $\lambda \in \mathbb{R}$ is a scalar and
  \[\mathbf{A}\vec{x}=\lambda\vec{x}\]
  then we say $\vec{x}$ is an \textbf{eigenvector} of $\mathbf{A}$ and
  we say that $\lambda$ is the \textbf{eigenvalue} of $\vec{x}$.
\end{defn}

Note that only square matricies have eigenvalues.

\begin{thm}
  The folllowing are logically equivalent
  \begin{align*}
    &\mathbf{A}\vec{x} = \lambda \vec{x} \\
    \iff& (\mathbf{A}-\lambda I) \vec{x} = 0 \\
    \iff& det(\mathbf{A} - \lambda I) = 0
  \end{align*}
\end{thm}

\begin{defn}
  Given $\mathbf{A} \in \mathbb{R}^{n \times n}$, we define the \textbf{characteristic polynomial} of $\mathbf{A}$ as
  \[p(\lambda) = \det(\mathbf{A}-\lambda I) = (\lambda_{1}-\lambda)(\lambda_{2}-\lambda)\cdots(\lambda_{n}-\lambda)\]
  where $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ are the $n$ eigenvalues of $\mathbf{A}$.
  The characteristic polynomial is an $n$-degree polynomial of $\lambda$.
\end{defn}

\begin{thm}
If $\mathbf{A} \in \mathbb{R}^{n \times n}$, then $\mathbf{A}$ has exactly $n$ eigenvalues (counting with multiplicity).
\end{thm}
\begin{defn}
  Given a fixed eigenvalue $\lambda\in \mathbb{R}$ of $\mathbf{A} \in \mathbb{R}^{n \times n}$, we define the \textbf{eigenspace} of $\lambda$ as
  \[\left\{\vec{x} : \mathbf{A}\vec{x} = \lambda\vec{x}\right\}\]
  Note that the eigenspace is a vector subspace of $\mathbb{R}^{n}$ because it is the nullspace of $(\mathbf{A}-\lambda I) \vec{x} = \vec{0}$. To find
  the eigenspace of $\lambda$, solve the homogenous linear equation $(\mathbf{A} - \lambda I)\vec{x} = \vec{0}$
\end{defn}

\section{May 4}
\begin{defn}
  If $A \in \mathbb{R}^{n \times n}$ and
  $B \in \mathbb{R}^{n \times n}$, we say A and B are \textbf{similar} if and only if there
  exists a matrix $P \in \mathbb{R}^{n \times n}$ such that
  \[PB = AP\]
  $P $ is called the change-of-basis matrix
\end{defn}

\begin{defn}
  We say a matrix
  $A \in \mathbb{R}^{n \times n}$
  is \textbf{diagonalizable}
  if it is similar to a diagonal matrix, i.e
  \[AP = PD\]
  equivalently
  \[A = PDP^{-1}\]
  for some diagonal matrix $D$
  the diagonal entries of $D$ are the eigenvalues of $A$.

  The diagonal matrix is the \textbf{eigenvector basis}.

\end{defn}
\begin{thm}
  $A$ is diagonalizable iff it has $n$ linearly independent eigenvectors.
\end{thm}
\begin{thm}
  two matricies are similar if and only if they have the same eigenvalues with the same multiplicties.
\end{thm}
% \begin{thm}
\begin{thm}
  If $A$ has $n$ distinct eigenvalues, then $A$ is diagonalizable.
  (warning: the converse does not always hold.)
\end{thm}


\begin{defn}
  A \textbf{stochastic matrix} represents a markov chain with a matrix.
  Please note that the COLUMN vectors represent the probability vectors.
  The sum of the column vectors must be 1.
  $a_{rc}$ represents the probability that we will go from state $r$ to state $c$.
  The top represents now, the right represents next.

  The \textbf{state vector} is a column vector which represnets the current probabilities.
\end{defn}

\begin{thm}
  $\lambda=1$ is an eigenvalue of all stochastic matricies. Use this fact to easily factor the characteristic polynomial of
  a stochastic matrix. This corresponds to the steady-state vector.
\end{thm}
\begin{ex}
  Here is an example of a stochastic matrix
  \[A = \begin{bmatrix} .9 & .2 \\ .1 & .8 \end{bmatrix}\]

  The steady state vector is
  \[\vec{x} = \begin{bmatrix} 2/3 \\ 1/3 \end{bmatrix}\]

  The eigenvalues are
  \[\lambda=1, \lambda = 0.7\]

  The diagonalization is
  \[\begin{bmatrix} .9 & .2 \\ .1 & .8 \end{bmatrix} = \begin{bmatrix} 2 & -1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & .7 \end{bmatrix}(1/3\begin{bmatrix} 1 & 1 \\ -1 & 2 \end{bmatrix})\]

  \[A^{\infty} = \begin{bmatrix} 2/3 & 2/3 \\ 1/3 & 1/3 \end{bmatrix}\]
\end{ex}

\begin{thm}
  It doesn't matter the inital state vector after a long time, all the columns of $M^{\infty}$ are the same.
\end{thm}
\begin{defn}
  The \textbf{steady state} vector is the fixedpoint of the markov chain, i.e. the probabilities don't change.
  If $\mathbf{A}\vec{x} = \vec{x}$
  then $\vec{x}$ is called the steady-state vector of $\mathbf{A}$.

  In other words, a steady state vector is an eigenvector for $\lambda =1$.

  To find the steady-state vector, solve the homogenous system
  \[(\mathbf{M}-\mathbf{I})\vec{x} = 0\]
\end{defn}
\section{May 8}


\subsection{Chapter 6 -- Least Squares}

Even if the equation $\mathbf{A}\vec{x} = \vec{b}$ is inconsistent, we can still project $\vec{b}$ onto $span(\mathbf{A})$. We have
that $\hat{b} = proj_{\mathbf{A}}(\vec{b})$.

\begin{defn}
  Given $\vec{u}, \vec{v} \in W$, the \textbf{inner product} is any function that satisfies the following axioms
  \begin{enumerate}
    \item $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$
    \item $(\vec{u} + \vec{v}) \cdot \vec{w} = \vec{u}\cdot \vec{w} + \vec{v}\cdot\vec{w} $
    \item $(c \vec{u}) \cdot \vec{v} = c(\vec{u} \cdot) = \vec{u} \cdot (c \vec{v})$
    \item $\vec{u} \cdot \vec{u} \ge 0$
  \end{enumerate}
\end{defn}

The most important inner product is called the \textbf{dot product} and is defined as
\[\vec{u} \cdot \vec{v} = \sum_{i=1}^{n} u_{i}v_{i}\]

\begin{defn}
  The \textbf{norm} of a vector is
  \[\|\cdot \| : \mathbb{R}^{n} \to \mathbb{R}\]
  \[\|\vec{u}\| := \sqrt{\vec{u} \cdot \vec{u}}\]
\end{defn}

\begin{defn}
  The \textbf{distance} between $\vec{u}$ and $\vec{v}$ is
  \[dist(\vec{u}, \vec{v}) := \| \vec{u} - \vec{v} \|\]
\end{defn}

\begin{thm}
  \textbf{orthogonality}: Two vectors are said to be orthogonal if and only if their dot products are $0$
  \[\vec{u} \bot \vec{v} \iff \vec{u} \cdot \vec{v} = 0\]
\end{thm}

\begin{thm}
  \textbf{pythagorean theorem}:
  \[\vec{u} \bot \vec{v}  \iff \|\vec{u}\|^{2} + \|\vec{v}\|^{2} = \|\vec{u}+\vec{v}\|^{2}\]
\end{thm}
\begin{thm}
  $\vec{0}$ is orthogonal to every vector.
\end{thm}

\begin{defn}
  \textbf{perpendicular to vector space}
  Let $W$ be a vector subspace of $V$ and let $\vec{u} \in V$. We say $\vec{u}$ is perpendicular to $W$ iff
  \[\vec{u} \bot W \iff \vec{u} \in W^{\bot} \iff \forall \vec{v} \in W, \vec{u} \bot \vec{v}\]
\end{defn}

\begin{defn}
  \textbf{orthogonal complement}: Let $W$ be a vector subspace of $V$. The orthogonal complement of $W$ is
  \[W^{\bot} := \left\{\vec{u} : \forall \vec{v} \in W, \vec{u} \bot \vec{v}\right\}\]
\end{defn}


\begin{defn}
  If $S \subseteq \mathbb{R}^{n}$, then $S$ is called an \textbf{orthogonal subset} of $\mathbf{R}^{n}$ iff
  \[\forall i,j \in [n] \qquad S_{i} \cdot S_{j} = \delta_{ij}\]
\end{defn}

\begin{thm}
  An orthogonal subset $S$ is linearly independent iff $\vec{0} \notin S$
\end{thm}

\begin{thm}
  If $\left\{\vec{v}_{1}, \ldots \vec{v}_{n}\right\}$ is an orthogonal basis of $V$, and $y \in V$, then
  \[\vec{y} =\sum_{i=1}^{n} c_{i}\vec{v}_{i}\]
  where
  \[c_{i} = \frac{\vec{y} \cdot \vec{v}_{i}}{\vec{v}_{i} \cdot \vec{v}_{i}}\]
\end{thm}
\begin{defn}
  A set $S \subset V$ is called an \textbf{orthogonal basis} of $V$ iff $S$ is a basis of $V$ and $S$ is an orthogonal subspace of $V$.
\end{defn}

\begin{thm}
  \[\forall \mathbf{A} \in \mathbb{R}^{r \times c} \qquad (Row(A))^{\bot} = Null(A)\]
  \[\forall \mathbf{A} \in \mathbb{R}^{r \times c} \qquad Col(A)^{\bot} = Null(A^{\top})\]
\end{thm}
\begin{thm}
  \textbf{orthogonal decomposition}:
  If $W$ is a vector subspace of $\mathbb{R}^{n}$, then every $y \in \mathbb{R}^{n}$ can be uniquely written as
  \[\vec{y} = \hat{y} + \vec{z}\]
  Where $\hat{y} \in W$ and $Z \in W^{\bot}$
\end{thm}

\section{May 10}
\begin{ex}
  Here is an application of orthogonal projection in math.

  Find the shortest possible distance from $(2,1)$ to the line passing through $(0,0)$ and $(1,-3)$?

  Let $\vec{p} = \langle 2,1 \rangle, \vec{q} = \langle 1, -3\rangle$.
  \[\hat{p} = proj_{q}(p) = \frac{p\cdot q}{q \cdot q} q = -\frac{1}{10}\langle 1 , -3\rangle\]
  The orthogonal decomposition is
  \[\vec{p} = \hat{p} + \vec{z}\]
  so
  \[\langle 2 , 1 \rangle = -\frac{1}{10}\langle 1, -3 \rangle + \frac{7}{10}\langle 3 , 1 \rangle\]
  So the distance is
  \[d = \|\vec{z} \| = \|\frac{7}{10}\langle 3, 1 \rangle\| = \frac{7}{\sqrt{10}}\]
\end{ex}

\begin{defn}
  We say $B$ is an \textbf{orthonormal basis} if
  \[\forall i,j \qquad b_{i}\cdot b_{j} = \delta_{ij}\]
  Basically all the vectors are unit vectors and all the vectors are pairwise orthogonal.

\end{defn}

\begin{defn}
  We say that $M$ is an \textbf{orthogonal matrix} iff
  \[M^{T}=M^{-1}\]
  Another way of saying this is
  \[M^{T}M=MM^{T}=I\]
\end{defn}
\begin{thm}
  If $M$ is an orthogonal matrix, then $M$ is always invertible.
\end{thm}
\subsection{6.4 -- Geometric Interpretation of Orthogonal Projection}
\begin{thm}
  \textbf{orthogonal decomposition}:
  If $W$ is a vector subspace of $\mathbb{R}^{n}$, then every $y \in \mathbb{R}^{n}$ can be uniquely written as
  \[\vec{y} = \hat{y} + \vec{z}\]
  Where $\hat{y} \in W$ and $z \in W^{\bot}$

  And if $W$ has an orthogonal basis $B = \left\{\vec{u}_{1}, \ldots, \vec{u}_{p}\right\}$, the coordinates of $\hat{y}$
  are given by
  \[\hat{y} = proj_{W}(\vec{y}) = proj_{\vec{u}_{1}}(\vec{y}) + \cdots + proj_{\vec{u}_{p}}(\vec{y}) = \sum_{i=1}^{p}proj_{\vec{u}_{i}}(\vec{y})\]

  Warning: this only works for an orthogonal basis.
\end{thm}

\begin{thm}
  \textbf{closest approximation theorem}: Basically, an orthogonal projection is the closest vector to the vector.

  Let $W$ be a vector subspace of $\mathbb{R}^{n}$ and let $\vec{y} \in \mathbb{R}^{n}$ then
  \[\forall \vec{v} \in W \qquad \| \vec{y} - proj_{W}(\vec{y}) \| \le \| \vec{y} - \vec{v} \|\]
\end{thm}


\begin{ex}
  Let $B = \left\{\begin{bmatrix} 3 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 2 \\ 0 \end{bmatrix} \right\}$ be an orthogonal basis of some subspace.
  Find the orthogonal decomposition of $\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$.\\
  Solution: \\

  \[\hat{y} = \frac{6}{10}\begin{bmatrix} 3 \\ 0 \\ 1 \end{bmatrix} + \begin{bmatrix} 0 \\ 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 9/5 \\ 2 \\ 3/5 \end{bmatrix}\]

  \[\begin{bmatrix} 1 \\ 2\\ 3 \end{bmatrix} = \begin{bmatrix} 9/5 \\ 2 \\ 3/5 \end{bmatrix} + \begin{bmatrix} -4/5 \\ 0 \\ 12/5 \end{bmatrix}\]
\end{ex}

\begin{thm}
  \textbf{Gram-Schmidt Procedure}:
  Given a vector subspace $W$ of $V$ and Given a basis $B = \left\{\vec{x}_{1}, \ldots, \vec{x}_{n}\right\}$ of $W$, then the set
    \begin{align*}
      \vec{v}_{1} &= \vec{x}_{1}\\
      \vec{v}_{2} &= \vec{x}_{2} - proj_{\{\vec{v}_{1}\}}(\vec{x}_{2}) \\
      \vec{v}_{3} &= \vec{x}_{3} - proj_{\{\vec{v}_{1}, \vec{v}_{2}\}}(\vec{x}_{3}) \\
      \vdots \\
      \vec{v}_{p} &= \vec{x}_{p} - proj_{\{\vec{v}_{1}, \vec{v}_{2}, \ldots, \vec{v}_{p-1}\}}(\vec{x}_{p})
    \end{align*}
    The set $V = \left\{\vec{v}_{1}, \vec{v}_{2}, \ldots, \vec{v}_{p}\right\}$ spans $W$ and is orthogonal.
\end{thm}
\section{May 11}
\subsection{6.5 -- Least Squares}

Here is a problem: If the system
\[\mathbf{A}\vec{x} = \vec{b}\]
has no solution, does it at least have a close solution
\[\mathbf{A}\hat{x} = \vec{b}\]


By projection,
\[\hat{b} = proj_{Col(\mathbf{A})}(\vec{x})\]

\begin{defn}
  If $\mathbf{A} \in \mathbb{R}^{m \times n}$ and $\vec{b} \in \mathbb{R}^{m}$, the \textbf{least squares solution} of the linear system
  \[\mathbf{A}\vec{x} = \vec{b}\]
  is the vector $\hat{b} \in \mathbb{R}^{n}$ such that
  \[\forall \vec{y} \in \mathbb{R}^{n} \qquad \| \vec{b} - \mathbf{A}\hat{x} \| \le \| \vec{b} - \mathbf{A} \vec{x}\]

  Basically, even if the linear system is inconsitent, $\hat{x}$ is the closest possible solution.

\end{defn}

\begin{thm}
  By the closest approximation theorem from yesterday, the least squares solutions are all the solutions to
  \[\mathbf{A}\hat{x} = proj_{Col(\mathbf{A})}(\vec{b})\]

  This method of calculating is very bad because you need an orthogonal basis.
\end{thm}

\begin{defn}
  For $\mathbf{A} \in \mathbb{R}^{m \times n}$,
  \[\mathbf{A}^{T}\mathbf{A}\]
  is called the \textbf{normal matrix} of $\mathbf{A}$.

  If $\mathbf{A}\vec{x} = \vec{b}$ is a linear system, then
  \[\mathbf{A}^{T}\mathbf{A} \vec{x} = \mathbf{A}^{T}\vec{x}\]
  is called the \textbf{normal equation}.
\end{defn}
\begin{thm}
  \textbf{calculating least-squares}: The least squares solutions of
  \[\mathbf{A}\vec{x} = \vec{b}\]
  are the same as the set of solutions to
  \[\mathbf{A}^{T}\mathbf{A} \vec{x} = \mathbf{A}^{T}\vec{b}\]
\end{thm}
\begin{ex}
  Find the least squares solution to the linear system
  \[\begin{bmatrix} 2 & 0 \\ 0 & 1 \\ 2 & 2 \end{bmatrix} \vec{x} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\]

  The normal equation is
  \[
\left(\begin{array}{rr}
8 & 4 \\
4 & 5
\end{array}\right)\hat{x} = \begin{bmatrix} 8 \\ 8 \end{bmatrix} \]
\end{ex}

\begin{ex}
  Given the points
  \[(1,1), (2,1), (3,2)\]
  find the least-squares regression line.

  We want the least-squares solution to
\[\begin{bmatrix} 1  & 1 \\ 2 & 1 \\ 3 & 1\end{bmatrix} \vec{x} = \begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix}\]
So
\[y = \frac{1}{2}x + \frac{1}{3}\]
\end{ex}
\section{May 15}
Here are some facts to remember:
\begin{enumerate}
  \item If the eigenvalues are distinct, the matrix is diagonalizable. (The converse is not necessarily true)
  \item If all the entries in a markov chain are positive, there is always a steady state (this is called the Perron-Frobenius Theorem)
\end{enumerate}

\begin{defn}
  If $V$ is a vector space, we say $V$ is an \textbf{inner product space} iff $V$ has an inner product defined
  \[\langle \cdot,\cdot \rangle : V \times V \to \mathbb{R}\]
  Such that $\langle \cdot ,\cdot \rangle$ satisfies 4 axioms:
  \begin{enumerate}
          \item \[\forall \vec{u}, \vec{v} \in V \qquad \langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u} \rangle\]
          \item \[\forall \vec{u}, \vec{v}, \vec{w} \in V \qquad \langle \vec{u} + \vec{v}, \vec{w} \rangle = \langle \vec{u} , \vec{w} \rangle + \langle \vec{v} , \vec{w} \rangle\]
          \item \[\forall c\in \mathbb{R} \vec{u}, \vec{v} \in V \qquad \langle c\vec{u} , \vec{v}\rangle = c\langle \vec{u}, \vec{v} \rangle\]
    \item \[\forall \vec{u}, \vec{v} \in V \qquad \langle \vec{u}, \vec{v} \rangle \ge 0\]
        \item \[\forall \vec{u } \in V \qquad \langle \vec{u}, \vec{u} \rangle = 0 \iff \vec{u} = \vec{0}\]
  \end{enumerate}
\end{defn}

\begin{ex}
  Here is an inner product on $\mathbb{P}_{n}$: Let $t_{0}, t_{1}, \ldots, t_{n}$  be $n$ distinct numbers.
  \[\forall p,q \in \mathbb{P}_{n} \qquad \langle p , q \rangle := p(t_{0})q(t_{0})+\cdots +p(t_{n})q(t_{n}) = \sum_{i=1}^{n}p(t_{i})q(t_{i})\]
\end{ex}
\section{May 18}

\begin{ex}
  Let
  \[M_{A} := \left\{ \mathbf{B}  : \forall \mathbf{B} \in \mathbb{R}^{q \times r}  \quad AB = 0\right\}\]
  \[M := \left\{\mathbf{A} : \forall \mathbf{A} \in \mathbb{R}^{n \times n} , \quad \det(\mathbf{A}) = 0\right\}\]
  \[S_{1} := \left\{ f : \forall f \in \mathbb{R} \to \mathbb{R}, \quad f(1) + f(2) = 0\right\}\]
  \[S_{2} := \{p : \forall p \in \mathbb{P}_{2} , \quad p(1) + 2p(3) = 0\}\]
  \[S_{3} := \{\mathbf{A} : \forall \mathbf{A} \in \mathbb{R}^{n \times n} , \det(\mathbf{A}) \neq 0\}\]

  \[S_{4} := \{(x,y,z) : \forall x,y,z \in \mathbb{R} , z = xy\}\]
  Determine which of these are vector spaces?
  $M_{A}$ is a vector space because it is a vector subspace of $\mathbb{R}^{q \times r}$.
  $M$ is not a vector space because it is not closed under addition.
  $S_{1}$ is a vector space because it is a vector subspace of $\mathbb{R} \to \mathbb{R}$.
  $S_{2}$ is a vector space because it a vector subspace of $\mathbb{P}_{2}$.
  $S_{3}$ is not closed under addition.
  $S_{4}$ is not closed under addition.
\end{ex}
\end{document}
