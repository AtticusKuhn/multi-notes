\documentclass[11pt]{article}
%  \usepackage{evan}
\usepackage{mathtools}
\usepackage{sagetex}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{sectsty}
% \usepackage{graphicx}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\newcommand{\Bold}[1]{#1}
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{Linear Algebra Notes}
\author{Atticus Kuhn}
\date{\today}


\begin{document}
\maketitle
\tableofcontents
\section{March 20}
\begin{defn}
  A \textbf{linear equation} of the variables $x_{1}, x_{2}, \ldots x_{n}$ is any equation of the form
  \[a_{1}x_{1}+a_{2}x_{2} + \cdots + a_{n}x_{n} = b\]
  where $a_{1}, \ldots a_{n}$ are called the coefficents and $b$ is called the constant.
  \end{defn}
  \begin{defn}
    A \textbf{system of linear equation} is a set of 1 or more linear equations.
    \end{defn}
    \begin{ex}
      \[x + 2y = 3\]
      is a linear system and

      \begin{align*}
        x-2y = 1 \\
        2x + 3y = 4 \\
        x + y =  5
        \end{align*}
        is a linear system.
        But
        \[5 \sqrt{x} - y=1\]
        is not a linear system.
      \end{ex}

      Given a linear system, we want to find all substitutions which satisfy the linear
      system.
      A \textbf{solution set} is a set of n-tuples, all of which are valid solutions to a given system of linear equations.
      Our central problem is: given a linear system, find its solution set.
      Big problems
      \begin{enumerate}
        \item Existance Question: Do there exist any solutions? If there are, we call the system \textbf{consistent}. If there are no solutions, we call the solution \textbf{inconsistent}.
              \item Uniqueness Question: How many solutions are there? If there is exactly 1 solution, we call that \textbf{unique}.
      \end{enumerate}
      \begin{thm}
        If a linear system has solutions, it either has 1 solution or infintely many solutions.
        A linear system cannot have 12 solutions.
      \end{thm}
      \begin{ex}
        \[x + 2y = 5, x + 4y = 6\]
        has solution
        \[x = -4, y=9/2\]
        This linear system corresponds to the matrix multiplication
        \[\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\]
        We can turn this into an \textbf{augmented matrix} to get
        \[
          \begin{bmatrix}
            1 & 2 & 5 \\
            3 & 4 & 6
            \end{bmatrix}
        \]
        We are allowed to perform row operations on the rows to get
\[
          \begin{bmatrix}
            1 & 2 & 5 \\
            0 & 1 & 9/2
          \end{bmatrix}
          \implies
          \begin{bmatrix}
            1 & 0 & -4\\
            0 & 1 & 9/2
            \end{bmatrix}
        \]
        So $(x,y) = (-4, 9/2)$. The solution is unique.
      \end{ex}

      \begin{thm}
        We are allowed to do these elementary row operations
        \begin{defn}
          We say two systems of equations are \textbf{equivalent}
          if their solutions sets are equal.
          \end{defn}
        \begin{enumerate}
          \item replacement: replace one row with the sum of itself and another row
          \item interchange: swap 2 rows
                \item scaling: multiply all entries in a  row by a constant
                \end{enumerate}
        \end{thm}
        The reason the elementary operations work is because they
        give an equivalent system, i.e. they do not change the solution set.

        Convince yourself that each of the elementary operations do not change the solution
        set.
        \begin{ex}
          Let's solve this system
          \begin{align*}
            kx + y + 2z &=1 \\
            x + z &= h \\
            y-z &= 1
            \end{align*}
            The solution is
            \[x=-3h/(k-3), y=\frac{(h+1)k-3}{k-3}, z = \frac{hk}{k-3}\]
            The system would have no solutions (inconsistent) iff $k=3$ and $h \neq 0$.
            The system has infinite solutions if $k=3$ and $h= 0$.
            The system has 1 solution if $k \neq 3$.
          \end{ex}
          \begin{defn}
            A matrix is in \textbf{echelon form} iff
            \begin{enumerate}
              \item All nonzero rows are above any rows of all zeroes
              \item each leading entry in a row is in a column to the right of the leading
                    endtry of the row above it
                \item All entries in a column below a leading entry are zeroes
            \end{enumerate}
          \end{defn}
          \begin{defn}
            A matrix is in \textbf{reduce row echelon form} iff
            \begin{enumerate}
              \item The leading entry in each nonzero row is $1$
                    \item each leading $1$ is the only nonzero entry in its column
                    \end{enumerate}
            \end{defn}
            \begin{ex}
              This matrix
              \[
                \begin{bmatrix}
                  * & * & 0 & 0 & 0 & 0 \\
                  0 & * & 1 & 0 & 0 & 0 \\
                  0 & 0 & * & 0 & 0 & *
                  \end{bmatrix}
                \]
                is in REF, but not in RREF.
              \end{ex}
              \begin{ex}
                This matrix
                \[\begin{bmatrix}
                    0 & 1  & 0 & 0 & 0 \\
                    0 & 0 & 2 & 0 & 0 \\
                    0 & 0 & 0 & 0 & 1 \\
                    0 & 0 & 0 & 0 & 0
                    \end{bmatrix}\]
                  is in REF, and it would be in RREF if we changed the $2$ to a $1$.
                \end{ex}

                \begin{thm}
                A given matrix has infinitely many equivalent REF, but only 1 RREF.
                \end{thm}
                \begin{ex}
                  Reduce this matrix
                  \[
                    \begin{bmatrix}
                      1 & -1 & -1 \\
                      0 & 2 & 1 \\
                      2 & 0 & -1
                      \end{bmatrix}
                  \]
                  Solution:
                  \[
\left(\begin{array}{rrr}
1 & 0 & -\frac{1}{2} \\
0 & 1 & \frac{1}{2} \\
0 & 0 & 0
\end{array}\right)
                  \]
   \end{ex}
 \begin{defn}
   If a matrix is in REF, the \textbf{pivots}
   are leading entries. The \textbf{pivot columns} are
   the columns which contain a pivot.
\end{defn}
\begin{defn}
  A \textbf{basic variable} is a variable which corresponds to a pivot column.
  A \textbf{free variable} is a variable which does not correspond to a pivot column.
\end{defn}
\begin{ex}
  Given the matrix
  \[\begin{bmatrix}
      1 & 0 & 2 & 0 & 4 \\
      0 & 1 & 3 & 0 & 5 \\
      0 & 0 & 0 & 1 & 6 \\
    0 & 0 & 0 & 0 & 0 \end{bmatrix}\]
This matrix is in RREF. Variables $x_{1}, x_{2}, x_{4}$ are basic variables
and variable $x_{3}$ is a free variable.
\end{ex}

If the righmost column of $A$ is a pivot column, then we cannot tell if $\mathbf{A}\vec{x} = \vec{b}$ is consistent or inconsistent.
\begin{thm}
  If the rightmost column of
  $\left[\mathbf{A} \quad \vec{b}\right]$ is a pivot column, then
  $\mathbf{A}\vec{x} = \vec{b}$ is not consistent.
\end{thm}

\begin{thm}
  Even if you have a free variable, that doesn't guarantee infintely many solutions.

  If you have a free variable and the system is consistent, then the system has infinitely many solutions.
\end{thm}

\begin{ex}
  Given that the RREF is
  \[\begin{bmatrix}
      1 & 2 & 3 & 0 &  0 &  6 \\
      0 & 0 & 0  & 1 & 0 & 7 \\
      0 & 0 & 0 & 0 & 1 & 8\end{bmatrix}\]
  Describe the solution set.
  It is
  \[
    \begin{bmatrix}
      x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \\ x_{5}
    \end{bmatrix}
    =
    x_{2} \begin{bmatrix} -2  \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}
    +
    x_{3} \begin{bmatrix} -3 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}
    +
    \begin{bmatrix} 6 \\ 0 \\ 0 \\ 7 \\ 8 \end{bmatrix}
  \]
  This solution set is infinitely big because it has free varaibles.
  This is called \textbf{parametric vector form} of the solution set.
\end{ex}
\section{March 22}
Let's review what we learned last session about matricies and linear systems.
\begin{enumerate}
  \item the RREF form of a matrix is unique (but the REF form is not).
  \item the elementary row operations do not affect the solution set of a linear system.
  \item pivot columns correspond to basic variables, non-pivot columns correpond to free variables.
  \item every linear system has either 0, 1, or $\infty$ solutions.
  \item an augmented matrix can be used to represent a linear system. It is the coefficent matrix concatenated with the constant vector.
  \item We can express the solution set of a linear system in parametric vector form.
  \item If a system is consistent and has a free variable, it has infinitely many variables (but even inconsistent systems may have free variables)
  \item row operations do not change pivot columns.
\end{enumerate}

\begin{ex}
  Let
  \[p = (0,1) \qquad q = (1,1) \qquad r = (2,2)\]
  Prove there exists no line with all 3 points.

  Prove there exists exactly one unique parabola with all 3 points
  \\
  Solution: \\

  The line has linear system
  \[1  = 0a + b \qquad 1 = a + b \qquad 2 = 2a + b\]
  \begin{align*}
    \begin{bmatrix}
      0 & 1 & 1 \\
      1 & 1 & 1 \\
      2 & 1 & 2 \\
    \end{bmatrix}
    \sim
\left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)
    \end{align*}
\end{ex}
So inconsistent.

For the parabola, it has the system
\[1 = 0a+0b+c \qquad 1 = a + b +c \qquad 2 = 4a + 2b + c\]
\begin{align*}
  \begin{bmatrix}
    0 & 0 & 1 & 1 \\
    1 & 1 & 1 & 1\\
    4 & 2 & 1 & 2 \\
  \end{bmatrix}
  \sim \\
\left(\begin{array}{rrrr}
1 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & -\frac{1}{2} \\
0 & 0 & 1 & 1
\end{array}\right)
\end{align*}
i.e.
\[y=\frac{1}{2}x^{2} - \frac{1}{2}x + 1\]
solution is unique because no free varaibles.
\subsection{1.3 -- Vector Equation}

\begin{defn}
  \textbf{matrix multiplication by vector}: Given
  \[A = [\vec{a_{1}}, \vec{a_{2}}, \vec{a_{3}}, \ldots, \vec{a_{n}}]\]
  and
  \[\vec{v} = \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \\ \vdots \\ v_{n} \end{bmatrix}\]
  then
  \[A\vec{v} = \vec{a_{1}}v_{1}+\vec{a_{2}}v_{2} + \cdots + \vec{a_{n}}v_{n} = \sum_{i=1}^{n} \vec{a_{i}}v_{i}\]
  We call this a \textbf{linear combination} of $a_{i}$. A linear combination is a
  sum of scalings of vectors, i.e. scale the vectors and then add them up.
\end{defn}

\begin{ex}
  \[\begin{bmatrix} 1 & 2 & 3 \\ 5 & 4 & 7 \end{bmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} a + 2b + 3c \\ 5a + 4b + 7c \end{bmatrix}\]
\end{ex}

\begin{defn}
  \textbf{span}: Given a set of vectors $S = \{\vec{v_{1}}, \vec{v_{2}}, \ldots, \vec{v_{n}} \}$, then $span(S)$ is the set of
  all linear combinations of the vectors. In other words, it is the set of all vectors reachable by using only the vectors in $S$.
  In equational form,
  \[span(S) = \left\{ \left .\sum_{i=1}^{n} c_{i} \vec{v}_{i} \;\right |\; \forall c_{i} \in \mathbb{R}\right\}\]
\end{defn}


\begin{ex}
  In $\mathbb{R}^{3}$,
  \[span(\{\vec{v}\}) = \{k\vec{v}  \quad | \quad k \in \mathbb{R}\}\]
  This set is a line going through the origin (unliness $\vec{v} = \vec{0}$, then it's just the origin).
  The $\vec{0}$ vector is always in the span set.

  \[span(\{\vec{u}, \vec{v}\})\]
  this is a plane passing through the origin (unless the vectors are linearly dependent, then it's a line). It could be origin, or line thru
  origin, or plane thru origin.

  \[span(\{\vec{u}, \vec{v}, \vec{w}\})\]
  is either the origin, or a line thru the origin, or a plane thru the origin, or the entire space.
\end{ex}

\begin{ex}
  \[A = \begin{bmatrix} 1  & -1 \\ 2 & 1 \\ 1 & 0 \end{bmatrix}, \qquad v = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}\]
  Is $Ax = b$ consistent?
  This is equivalent to asking if $b$ is a linear combination of the columns of $A$ and it is equivalent to asking
  if $b$ is an element of the spanning set of the columns of $A$.

  This is not consistent because
  \[\left(\begin{array}{rrr}
1 & -1 & 0 \\
2 & 1 & 1 \\
1 & 0 & 2
\end{array}\right)\sim \left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)\]

Thus, $b \not\in span(A)$
\end{ex}

\begin{ex}
  \[x + 2y + 3z = 4 \qquad 5x + 6y + 7z = 8 \]
  So
  \[\begin{bmatrix} 1 & 2 & 3 \\ 5 & 6 & 7 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 4 \\ 8 \end{bmatrix}\]
\end{ex}

\begin{thm}
  Asking whether the system $Ax = b$ is consistent is the same as asking if $b \in span(A)$
\end{thm}
\section{March 23}
\subsection{1.4 -- Matrix Equation}

\begin{ex}
  Let
  \[A =\begin{bmatrix} 1 & 4 & 5 \\ -3 & -1 & -14 \\ 2 & 8 & 10 \end{bmatrix}\]
  Let $b \in \mathbb{R}^{m}$. Is $Ax=b$ consistent for any $b$? \\
  Solution: \\
  Not consistent because if you rref
  \[
\left(\begin{array}{rrrr}
1 & 4 & 5 & b_{1} \\
-3 & -11 & -14 & b_{2} \\
2 & 8 & 10 & b_{3}
\end{array}\right) \]
you get the $-2b_{1} + b_{3} = 0$.

Now, find the set spanned by the column vectors of $A$. \\

(note that the spanning set is NOT $\mathbb{R}^{3}$, because the columns have a
rank of 2).
This is
\[span(A) = \left \{\left . \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \end{bmatrix} \; \right\vert -2b_{1} + b_{3} = 0\right\} = \left\{ b_{2} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} + b_{3} \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} \vert \forall b_{2}, b_{3} \in \mathbb{R}\right\} = span\left(\left\{\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\2\end{bmatrix}\right\}\right)\]
You can do this is Sage with
\begin{sageblock}
 A =  span(column_matrix([[1,4,5],[3,-11,-14], [2,8,10]]))
\end{sageblock}
\end{ex}
\[A = \sage{A}\]
\begin{thm}
  The following statements are logically equivalent:
  Let $A$ be an $m \times n$ matrix and let $b$ be an $m \times 1$ vector.
  \begin{enumerate}
      \item for all $b \in \mathbb{R}^{m}$, $Ax = b$ has a solution.
      \item $span(A) = \mathbb{R}^{m}$
    \item The columns of $A$ span $\mathbb{R}^{m}$
      \item $A$ has 1 pivot position in every row.
      \item $b$ can be written as a linear combination of the columns of $A$.
      \item $Ax = b$ is consistent for all $b \in \mathbb{R}^{m}$.
  \end{enumerate}
\end{thm}

Make sure you don't mess up the details. We want a pivot in every ROW of $A$, not every column of $A$.
\begin{ex}
  Let's apply theorem 7.
  \[span\left(\left\{ \begin{bmatrix} 1 \\ 2 \end{bmatrix}\right\}\right) \neq \mathbb{R}^{2}\]
  because it doesn't have a pivot in the second row.
\end{ex}

\begin{ex}
  \[span\left(\left\{ \begin{bmatrix} 1 \\ 1 \end{bmatrix},\begin{bmatrix} 1 \\ 0 \end{bmatrix},\begin{bmatrix} 2 \\ 1 \end{bmatrix}\right\}\right) = \mathbb{R}^{2}\]
  because there's a pivot in every row.
  Note that there is not a pivot in every column, but the theorem requires a pivot in every row.
\end{ex}

Another mistake people make is thinking that the theorem applies if there is a pivot in every row of $\left [ A \quad b\right ]$. We are only concerned
about a pivot in every row of $A$.
\section{March 27}
\subsection{Homogenous System}
\begin{defn}
  The linear system $Ax=b$ is called \textbf{homogenous} iff $b=\vec{0}$. A homogenous linear system is any system of the form $Ax = \vec{0}$.
\end{defn}
\begin{thm}
  A homogenous linear system is always consistent because \[\mathbf{A}\vec{0} = \vec{0}\]
\end{thm}
\begin{defn}
  We call $x = \vec{0}$ the \textbf{trivial solution} to the linear system $Ax = \vec{0}$.
\end{defn}
\begin{thm}
  A homogenous linear system has infinite nontrivial solutions if and only if it contains free variables.
\end{thm}
\begin{ex}
  consider this homogenous linear system
  \[\begin{bmatrix} 1 & 2 \\ 3  & 6 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\]
  The solution set is
  \[span\left\{\begin{bmatrix} -2 \\ 1 \end{bmatrix}\right\}\]
\end{ex}

\begin{ex}
  consider this homogenous linear system
  \[\begin{bmatrix} 1 & 2 \\ 3  & 6 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 4 \\ 12 \end{bmatrix}\]
  Then
  \[\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} -2y + 4 \\ y \end{bmatrix}\]
  This solution set cannot be written as a span because it doesn't contain $\vec{0}$.
\end{ex}

The solution set of a homogenous system can always be expressed as the span of some vectors.

\begin{ex}
  Here are 2 systems to consider.
  \[S_{1} : x + 2y + 3z = 0\]
  \[S_{2} : x + 2y + 3z = 4 \]
  Compare and contrast the solution sets. \\
  Solution: \\
  The solution set for $S_{1}$ is $span \left\{\begin{bmatrix}-3 \\ 0 \\ 1\end{bmatrix}, \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}\right\}$.
  The solution set for $S_{2}$ is
  \[\left \{\left.\begin{bmatrix} -2y-3z + 4 \\ y \\ z \end{bmatrix} \right| \forall y,z \in \mathbb{R}\right\}\]
\end{ex}

\begin{thm}
  The solution set to $Ax = b$ is parallel to the solution set of $Ax = 0$. You can think of the solution set of $Ax=b$ as being
  a constant vector translation from the solution set of $Ax=0$.

  Suppose $p$ is any solution to the system $\mathbf{A}\vec{x} = \vec{b}$. Then, any other solution to $\mathbf{A}\vec{x} = \vec{b}$ can
  be written as $\vec{p} + \vec{v}_{h}$, where $\vec{v}_{h}$ is any solution to $\mathbf{A}\vec{x} = \vec{0}$.
\end{thm}
Warning: you can only think of the solution set of $\mathbf{A}\vec{x} = \vec{b}$ as a translation of the solution set of $\mathbb{A}\vec{x} = \vec{0}$ is the linear system is consistent. Otherwise, it's solution set is the
empty set.
\subsection{1.6 -- Applications}
There are a bunch of applications of matricies in the textbook.
\begin{ex}
  Find the general traffic pattern in the freeway network shown in the figure \\
  Describe the general traffic pattern when the road $x_{1}$ is closed. \\
  When $x_{4} = 0$ what is the minimum value of $x_{1}$? \\
  Solution:\\
  For the solution, we use the general principle that (flow in) - (flow out) = 0.
  \[A = \begin{bmatrix} 1 & 0 & -1 & -1 & 0 & 40 \\
          -1 & -1 & 0 & 0 & 0 & -200 \\
          0 & 1 & 1 & 0 & -1 & 100 \\
          0 & 0 & 0 & 1 & 1 & 60\end{bmatrix}\]
\begin{align*}
  A &: x_{1} &= x_{3} + x_{4} + 40\\
  B &: 200 &= x_{1} + x_{2} \\
  C &: x_{2} + x_{3} &= 100 + x_{5} \\
  D &: x_{4} + x_{5} &= 60 \\
\end{align*}
This reduces to
\[\left(\begin{array}{rrrrrr}
1 & 0 & -1 & 0 & 1 & 100 \\
0 & 1 & 1 & 0 & -1 & 100 \\
0 & 0 & 0 & 1 & 1 & 60 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right)\]
So $x_{1}, x_{2}, x_{4}$ are basic variables and $x_{3}, x_{5}$ are free variables. The solution set is
\[\left\{\begin{bmatrix} x_{3} - x_{5} + 100 \\ -x_{3} + x_{5} + 100 \\ x_{3} \\-x_{5} + 60 \\ x_{5}\end{bmatrix} | \forall x_{3}, x_{5} \in \mathbb{R}\right\}\]

If $x_{4} = 0$, then $x_{5} = 60$, so we get the solution set

\[\left\{\begin{bmatrix} x_{3} +40 \\ -x_{3} +   160 \\ x_{3} \\ 0  \\ 60\end{bmatrix} | \forall x_{3} \in [0, 160]\right\}\]

The minimum value of $x_{1}$ is $40$.
\subsection{1.7 -- Linear Independence}
\end{ex}
\begin{defn}
  Given a set of vectors $\vec{v}_{1}, \vec{v}_{2}, \vec{v}_{3}, \ldots$, we say that the set is \textbf{linearly independent}
  if and only if the only solution to
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + c_{3}\vec{v}_{3} + \cdots = \vec{0} \qquad c_{i} \in \mathbb{R}\]
  is the trivial solution.

  We say the set is \textbf{linearly dependent} if and only if there is a solution to the equation
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + c_{3}\vec{v}_{3} + \cdots = \vec{0}\qquad c_{i} \in \mathbb{R}\]
  which is not the trivial solution.
\end{defn}

\begin{thm}
  The columns of $\mathbb{A}$ are linearly independent if and only if the only solution to
  \[\mathbf{A}\vec{x} = \vec{0}\]
  is the trivial solution $\vec{x} = \vec{0}$.
\end{thm}
If one vector lies in the plane spanned by the other two, then the vectors are linearly dependent.
\begin{thm}
  To find out if a set of vectors is linearly dependent, RREF their matrix.
\end{thm}
\section{March 29}
No class due to doctor's appointment
\section{March 30}
\subsection{1.7 -- Linearly Indepedence}
\begin{defn}
  A \textbf{linearly independent set} is a set of vectors $\left\{\vec{v}_{1}, \vec{v}_{2}, \ldots \vec{v}_{n}\right\}$ such that
  \[c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + \cdots c_{3}\vec{v}_{3} = 0 \iff c_{1} = c_{2} = c_{3} = \cdots = 0 \]
  In other words, we say the columns of $\mathbf{A}$ are linearly independent iff
  \[\mathbf{A}\vec{x} = \vec{0} \iff \vec{x} = \vec{0}\]
\end{defn}

There are some special cases to easily determine if a set is linearly independent or dependent.
\begin{enumerate}
  \item The empty set $\{\}$ is linearly independent.
  \item The singleton set $\{\vec{v}\}$ is linearly independent if and only if $\vec{v} \neq \vec{0}$
  \item The two-element set $\{\vec{v}_{1}, \vec{v}_{2}\}$ is linearly independent iff $\not\exists k \in \mathbb{R}, \vec{v}_{1} = k\vec{v}_{2}$.
  \item Given a set of $p$ vectors $\{\vec{v}_{1}, \vec{v}_{2}, \ldots, \vec{v}_{p}\}$ in $\mathbb{R}^{n}$ and $p > n$, then the set is linearly dependent.
  \item If a set contains the $\vec{0}$ vector, then it is linearly dependent.
\end{enumerate}

Practically, to find out if vectors are linearly independent, RREF the matrix. If the RREF has a pivot in every row, then the vectors are linearly independent. Otherwise, the vectors
are linearly dependent.

\begin{ex}
  Are the column vectors in this matrix linearly independent? Just RREF it.
\end{ex}
\subsection{1.8 -- Linear Transformations}
\begin{defn}
  A \textbf{transformation} is a function $T : \mathbb{R}^{n} \to \mathbb{R}^{m}$. We call $\mathbb{R}^{n}$ the domain
  and we call $\mathbb{R}^{m}$ the codomain.
\end{defn}

\begin{defn}
  Given an $m \times n$ matrix $\mathbf{A}$, a \textbf{Matrix Transformation} is a linear transformation
  \[T: \mathbb{R}^{n} \to \mathbb{R}^{m}\]
  \[T(\vec{x}) = \mathbf{A}\vec{x}\]
  We call $\mathbf{A}$ the \textbf{standard matrix} of $T$.
\end{defn}

\begin{ex}
  For example, given the matrix
  \[\mathbf{A} = \begin{bmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{bmatrix}\]
  The matrix transformation of $\mathbf{A}$ is
  \[T : \mathbb{R}^{2} \to \mathbb{R}^{3}\]
  \[T\left(\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}\right) = \begin{bmatrix} x_{1} - 3x_{2} \\ 3x_{1} + 5x_{2} \\ -x_{1} + 7x_{2} \end{bmatrix}\]
\end{ex}

\begin{defn}
  Given a transformation $T: \mathbb{R}^{n} \to \mathbb{R}^{m}$,
  we say a transformation is a \textbf{linear transformation} iff it satisfies 2 axioms:
  \begin{enumerate}
    \item Vector addition: \[T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v}) \qquad \forall \vec{u}, \vec{v} \in \mathbb{R}^{n} \]
    \item Scaling: \[T(c\vec{v}) = c T(\vec{v}) \qquad \forall c \in \mathbb{R} \quad \forall \vec{v} \in \mathbb{R}^{n}\]
  \end{enumerate}
\end{defn}

\begin{thm}
  If $T: \mathbb{R}^{n} \to \mathbb{R}^{m}$  is a linear transformation, then
  \[T(\vec{0}_{n}) = \vec{0}_{m}\]
  because of the scaling rule.
\end{thm}

By the contrapositive, if $T(\vec{0}) \neq \vec{0}$, then $T$ is not a linear transformation.

\begin{thm}
  All matrix transformations are also linear transformations. Given a linear transformation and a basis, there is a unique matrix transformation corresponding to
  the linear transformation.

  In other words, a matrix transformation is basically the same thing as a linear transformation
\end{thm}

\begin{thm}
  The derivative $\frac{d}{dx}$ from the set of differentiable functions to functions. To see why, it satisfies the axioms
  \[\frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}(f(x)) + \frac{d}{dx}(g(x))\]
  \[\frac{d}{dx}(c f(x)) = c \frac{d}{dx}(f(x))\]
\end{thm}
\section{April 12}

\begin{thm}
  If $ T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ is a linear transformation, then there exists a matrix $A$ called the
  \textbf{standard matrix} defined by
  \[A = [T(e_{1}), T(e_{2}), T(e_{3}), T(e_{4}), \ldots, T(e_{n})]\]
  Where $e_{1}, e_{2}, \ldots, e_{n}$ are the \textbf{basis vectors}.
\end{thm}


The way to see this is that any arbitrary vector $\vec{v} \in \mathbb{R}^{n}$ can be written as a linear combination of the basis vectors as
$\vec{v} = c_{1}\vec{e}_{1} + \cdots c_{n}\vec{e}_{n}$.


\begin{ex}

  The matrix of a  vertical reflection in $\mathbb{R}^{2}$ is
  \[A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\]

  The matrix of a horizontal contraction (or expansion) by a factor of $k$ is
  \[A = \begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}\]


  The matrix of the projection onto the $y$ axis is
  \[A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}\]


  The matrix of the horizonal shear by a factor of $k$ is
  \[A = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}\]


  The matrix of a $\theta$ counter-clockwise about the origin is
  \[A = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} \]
\end{ex}

\begin{defn}
  Let $f : X \to Y$.
  $f$ is called \textbf{surjective} iff every element of $Y$ is hit.

  $f$ is called \textbf{injective} iff $f(x_{1}) = f(x_{2}) \implies x_{1} = x_{2}$.

  Surjective functions are also called ``onto functions''.
  Injective functions are also called ``one-to-one functions''
\end{defn}

\begin{defn}
  We say $f: X \to Y$ is \textbf{bijective} iff it is both surjective and injective.
\end{defn}


\begin{thm}
  Let $T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ be a linear transformation, and let $T$ have standard matrix $A$.

  $T$ is injective/one-to-one iff
  the equation $\textbf{A}\vec{x} = \vec{0}$ only has the solution $\vec{x} = \vec{0}$.
  This means that $A$ has a pivot in every column.


  In other words, $T(\vec{x}) = \vec{0} \implies \vec{x} = 0$.

  The following are equivalent
  \begin{enumerate}
    \item $T$ is an injective/one-to-one function
    \item $A$ has a pivot in every column
    \item $A$ has no free variables
    \item $T(\vec{x}) = \vec{0} \implies \vec{x} = \vec{0}$
    \item The columns of $\mathbf{A}$ are linearly independent.
  \end{enumerate}
\end{thm}


\begin{thm}
  Let $T : \mathbb{R}^{n} \to \mathbb{R}^{p}$ be a linear transformation with matrix $\mathbf{A}$. The follow are equivalent

  \begin{enumerate}
    \item $T$ is a surjection / onto function
    \item $A$ has a pivot in every row
    \item The system $\mathbf{A} \vec{x} = \vec{b}$ is consistent for every $\vec{b} \in \mathbb{R}^{p}$
    \item $span(A) = \mathbb{R}^{p}$.
  \end{enumerate}
\end{thm}


\begin{ex}
  Let $T: \mathbb{R}^{3} \to \mathbb{R}^{2}$ defined by
  \[T(\vec{x}) = \begin{bmatrix} 1  & -1 & 3 \\ -2 & 2 & -6 \end{bmatrix}\vec{x}\]

  Find $Range(T)$? \\
  Solution: \\
  $Range(T) = span(\begin{bmatrix} 1 \\ -2 \end{bmatrix})$\\
  Find the set of all pre-images of $\begin{bmatrix} 2 \\ -4 \end{bmatrix}$?\\
  Solution: \\
  \[S = \left\{ k\begin{bmatrix}1 \\ 1 \\ 0 \end{bmatrix} + l \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix}  + \begin{bmatrix} 2 \\ 0 \\ 0 \end{bmatrix} | \forall k,l \in \mathbb{R}^{3}\right\} = span\left(\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix}\right) + \begin{bmatrix} 2 \\ 0 \\ 0 \end{bmatrix}\]\\
  Is $T$ onto/surjective? \\
  Solution: \\
  no.\\
  Is $T$ one-to-one/injective? \\
  Solution: \\
  no.

\end{ex}
\section{April 13}


\begin{defn}
  Given a matrix $A$, we call $A^{-1}$ the \textbf{inverse} of $A$ iff
  $AA^{-1} = I$
  If $A$ has an inverse, then $A$ is called \textbf{invertable}.

\end{defn}

If $A$ is invertiable, then $A^{-1}$ is its unique inverse.


We only talk about inverses in the context of square matricies.

For a $1 \times 1$ matrix, the inverse is
\[a^{-1} = \frac{1}{a}\]

For a $2 \times 2$ matrix, the inverse is

\[\begin{bmatrix} a & b \\ c & d \end{bmatrix} ^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}\]


\begin{thm}
  If $A$ is invertable, then the system
  \[\mathbf{A}\vec{x} = \vec{b}\]
  has the unique solution
  \[\vec{x} = \mathbf{A}^{-1}\vec{b}\]
\end{thm}

\begin{ex}
  Solve this system
  \[\begin{bmatrix} 1 & 2 \\ 3 & 4\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix} \]

\end{ex}


\begin{thm}
  \[(A^{-1})^{-1} = A\]
  \[(AB)^{-1} = B^{-1}A^{-1}\]
  \[(A^{T})^{-1} = (A^{-1})^{T}\]
\end{thm}

\begin{thm}
  A matrix $A$ is invertible if and only if it is row-equivalent (by elementary row operatins) to
  the identity matrix $I$.
\end{thm}


\begin{thm}
  \textbf{Finding Inverse}: To find the inverse on the $n\times n$ matrix $A$, perform row operations $[A \quad I_{n}]$
  to get to $[I_{n} \quad A]$
  In other words
  \[[A \quad I_{n}] \sim [I_{n} \quad A]\]
\end{thm}

\begin{ex}

  \begin{align*}
    \begin{bmatrix} 2 & 0 & 0 & 1 & 0 & 0 \\ -3 & 0 & 1 & 0 & 1 & 0 \\ 0 & 1  & 0 & 0 & 0 & 1 \end{bmatrix} \\
    &\sim
\left(\begin{array}{rrrrrr}
1 & 0 & 0 & \frac{1}{2} & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & \frac{3}{2} & 1 & 0
\end{array}\right)
  \end{align*}
\end{ex}



\begin{thm}
  \textbf{Invertible Matrix Theorem}:
  The following statements are all logically equivalent
  \begin{enumerate}
    \item $A$ is an invertiable $n \times n$ matrix
    \item $A \sim I_{n}$
    \item $A$ has $n$ pivot positions
    \item The only solution $\mathbf{A}\vec{x} = \vec{0}$ is the trivial solution $\vec{x} = \vec{0}$
    \item The columns of $A$ form a linearly inpendent set/basis of $\mathbb{R}^{n}$
    \item The linear transformation $T(\vec{x}) = \mathbf{A}\vec{x}$ is one-to-one/injective
    \item The linear transformation $T(\vec{x}) = \mathbf{A}\vec{x}$ is onto/surjective
    \item The equation $\mathbf{A}\vec{x} = \vec{b}$ has exactly one solution $\vec{x} = \mathbf{A}^{-1}\vec{b}$ for all $\vec{b}\in \mathbb{R}^{n}$

  \end{enumerate}
\end{thm}
\subsection{Chapter 3 -- Determinants}

\begin{defn}
  Given a matrix $A$, the \textbf{sub-matrix} $A_{ij}$ is the matrix obtained by deleting row $i$ and column $j$
\end{defn}

\begin{thm}
  \textbf{Laplace Expansion}: The determinant can be calculated by
  \[|A| = \sum_{i=1}^{n} (-1)^{i+j} a_{ij}|A_{ij}|= \sum_{j=1}^{n} (-1)^{i+j} a_{ij}|A_{ij}|\]

  Some people call $(-1)^{i+j}|A_{ij}| = c_{ij}$ the \textbf{co-factor}.
\end{thm}

Note that Laplace expansion is very slow, it has time complexity $O(n!)$. If you want to calculate the determinant quickly, elementary row operations can
calculate the determinant in $O(n^{3})$.

\begin{thm}
  If $A$ is a triangular matrix, then $|A|$ is just the product of the diagonal entries.
  \[|A| = \prod_{i=1}^{n}a_{ii}\]
\end{thm}

It is a common mistake that $A \sim B \implies |A| =  |B|$, but this is false.

\begin{thm}
  Each of the elementary row operations has an effect on the determinant\\
  \textbf{scaling}: multiplying a row by $k$ multiplies the determinant by $k$ \\
  \textbf{interchange}: swapping two rows multiplies the determinant by $-1$ \\
  \textbf{replacement}: adding a row to another row doesn't change the determinant \\
\end{thm}

\begin{thm}
  \textbf{Properties of determinant}
  \[|AB|=|A||B|\]
  \[|A^{-1}|=|A|^{-1}\]
  \[|A^{T}|=|A|\]
\end{thm}

\section{April 17}

Let's review determinant.
\begin{align*}
  \begin{vmatrix} d-2g & e-2 & g-2i \\
    3a  & 3b & 3c \\
    -a  + 4g & -b + 4h & -c + 4i \end{vmatrix} \\
  &= -3 \cdot 4 \cdot \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix}
\end{align*}
\begin{thm}
  \textbf{Cramers Rule}: If $A$ is an invertiable $n \times n$ matrix, then the solution to the linear system
  \[\mathbf{A}\vec{x} = \vec{b}\]
  has entries
  \[\vec{x}_{i} = \frac{\det \mathbf{A}_{i}(\vec{b})}{\det \mathbf{A}}\qquad \forall i \in \{1,2,\ldots , n\} \]
  Where
  \[\mathbf{A}_{i}(\vec{b}) := [\vec{a}_{1} \ldots \vec{b} \ldots \vec{a}_{n}]\]
\end{thm}

\begin{thm}
The determinant is the (signed) volume or area of the parallepiped or parallelogram with edges given by the columns of $\mathbf{A}$.
\end{thm}

\subsection{Practice Knowledge Check Chapter 1}
\begin{enumerate}
  \item Consider the following System \[x + y + z = 2 \quad x - y + z = h \quad x - y + kz = 3 \]
        \begin{enumerate}
          \item apply elementary row oeprations to write in echelon form.
                \[\begin{bmatrix} 1 & 1 & 1 & 2 \\ 0 & -2 & 0 & h-2 \\ 0 & 0 & 0 & 3-h \end{bmatrix}\]
          \item Determine the values of $k, h$ such that the system has 0, 1, or $\infty$ solutions?
                $0$ solutions iff $k=1 \land h \neq 3$. $1$ solution iff $k \neq 1$, $\infty$ solutions
                iff $k=1 \land k=3$
          \item When the system has $\infty$ solutions, find the general solution in parametric vector form
                \[\begin{bmatrix} 5/2 \\ -1/2 \\ 0 \end{bmatrix} + span \begin{bmatrix} -1 \\ 0 \\1 \end{bmatrix}\]
          \item describe the above set geometrically?
                A line passing through $\begin{bmatrix} 5/2 \\ -1/2 \\ 0 \end{bmatrix}$ and parallel to $\begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$
                in $\mathbb{R}^{3}$
        \end{enumerate}
         \item Let $\mathbf{A} = \begin{bmatrix} 1 & -1 & -1 \\ 0 & 2 & 1 \\ 2 & - & -1 \end{bmatrix}$
        \begin{enumerate}
          \item Do the columns of $A$ span $\mathbb{R}^{3}$?
                No
          \item Find the span of the columns of $A$?
                \[span \left\{\begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} , \begin{bmatrix} -1  \\ 2 \\ 0 \end{bmatrix}\right\}\]
          \item Are the columns of $A$ linearly independent?
                No
          \item Write a dependence relation among the columns of $A$?
                \[\vec{a}_{3} = -\frac{1}{2}\vec{a}_{1} + \frac{1}{2}\vec{a}_{2}\]
          \item Geometrically describe $span \left\{a_{1}, a_{2}, a_{3}\right\}$?
                A place in $\mathbb{R}^{3}$ passing through the origin $\begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}$, and
                $\begin{bmatrix} -1 \\ 2 \\ 0 \end{bmatrix}$.
        \end{enumerate}
        \item Let $T : \mathbb{R}^{3} \to \mathbb{R}^{2}$ be a linear transformation that maps $(1,1,1) \mapsto (0,1)$, $(0,1,1) \mapsto (1,1)$, and $(0,1,1) \mapsto (1,-1)$
        \begin{enumerate}
          \item Is $T$ a matrix transformation?\\
                Yes
          \item Find the matrix of $T$?\\
                \[T(\vec{x}) = \begin{bmatrix} -1 & 1 & 0 \\ 2 & 1 & -2 \end{bmatrix} \vec{x}\]
          \item Find the image of $(x,y,z)$?\\
                \[\begin{bmatrix} -x+y \\ 2x+y-2z \end{bmatrix}\]
          \item Prove that $T$ is a linear transformation?\\
                \[T(\vec{a} + \vec{b}) = T(\vec{a} + \vec{b})\]
                \[T(c\vec{a}) = cT(\vec{a})\]
          \item Find the pre-image of $(1, 4)$ \\
                \[S = \left\{\left. \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}  + \begin{bmatrix} 2/3 \\ 2/3 \\ 1 \end{bmatrix} z \right | \forall z \in \mathbb{R}\right\}\]
          \item Is $T$ one-to-one/injective?\\
                no\\
          \item Is $T$ onto/surjective? \\
          yes
        \end{enumerate}

\end{enumerate}

\section{April 20}
\begin{defn}
  A set $V$ is called a \textbf{vector space} iff it satisfies the following axioms
  \begin{align*}
    \vec{u} + \vec{v} &= \vec{v} + \vec{u} \\
    (\vec{u} + \vec{v}) + \vec{w} &= \vec{u} + (\vec{v} + \vec{w})\\
    \vec{v} + \vec{0} &= \vec{v} \\
    \vec{u} + (-\vec{u}) &=0 \\
    c(\vec{u} + \vec{v}) &= c\vec{u} + c\vec{v} \\
    (c+d)\vec{u} &= c\vec{u}+d\vec{u}\\
    c(d\vec{u}) &= (cd)\vec{u} \\
    1\vec{u} &= \vec{u}
  \end{align*}
\end{defn}

Note that the empty set is \emph{not} a vector space because it does not contain $\vec{0}$.


The \textbf{zero space} $\{\vec{0}\}$ is a vector space.

\begin{ex}
  Examples of vector spaces
  \begin{enumerate}
    \item the zero space $\{\vec{0}\}$.
    \item $\mathbb{R}^{n}$ for any integer $n$.
    \item $\mathbb{P}$, the set of polynomials with real coefficients
    \item $\mathbb{P}_{n}$, the set of polynomials with real coefficients with degree at most $n$.
    \item signals: $\mathbb{S}$ the set of lists of real numbers that extend infinitely in both directions $(\ldots, x_{-1}, x_{0}, x_{1}, \ldots)$
    \item function space: real valued functions from any set $X$ to the real numbers $\mathbb{R}^{n}$
    \item Any plane passing through the origin.
  \end{enumerate}
\end{ex}

\section{April 24}
Nothing happend.
\section{April 26}
Here are some more examples of vector spaces
\begin{enumerate}
  \item The set of $m \times n$ matricies
  \item The set of integrable functions from a set $X$ to $\mathbb{R}$.
  \item For a fixed $m \times n$ matrix $B$, the set $\left\{ \mathbf{A} | \forall \mathbf{A} \in \mathbb{R}^{p \times m} ,  \mathbf{A}\mathbf{B} = \mathbf{0}\right\}$ (because it is the kernel/nullspace of $\mathbf{B}$).
  \item For a fixed $m \times n$ matrix $B$, the set $\left\{\mathbf{A} | \forall \mathbf{A} \in \mathbb{R}^{p \times m} , \mathbf{AB} = \mathbf{A}\right\}$ (because it is an invariant subspace)
\end{enumerate}

\begin{defn}
  Given a vector space $V$, we say that a set $W$ is called a \textbf{vector subspace} of $V$ if and only if
  \begin{enumerate}
    \item $W \subseteq V$
    \item $\vec{0} \in W$
    \item addition is closed under $W$, meaning that $\forall \vec{u},\vec{v} \in W \implies (\vec{u} + \vec{v}) \in W$
    \item $W$ is closed under scalar multiplication, meaning that $\forall c \in \mathbb{R} \quad \forall \vec{u} \in W \implies k\vec{u} \in W$.
  \end{enumerate}
\end{defn}


\begin{ex}
  Here is a list of examples of vector subspaces
  \begin{enumerate}
    \item For any vector space $V$, the zero subspace $\{\vec{0}\}$ is a subspace of $V$
    \item $\left \{ \begin{bmatrix} a \\ b \\ 0 \end{bmatrix} | \forall a , b \in \mathbb{R} \right\}$ is a vector subspace of $\mathbb{R}^{3}$
    \item $\left\{ \begin{bmatrix} x \\ y \\ x + y \end{bmatrix} | \forall x , y \in \mathbb{R} \right\}$ is a vector subspace of $\mathbb{R}^{3}$
\end{enumerate}
\end{ex}
\begin{thm}
  If $W$ is a vector subspace of $V$, then $W$ is automatically a vector space (you don't need to check the axioms).
\end{thm}

\begin{thm}
  If $V$ is a vector space and $S \subseteq V$, then $span(S)$ is a vector subspace of $V$.
\end{thm}


\begin{defn}
  Given a linear transformation $T : V \to W$, we define the \textbf{null space} as
  \[Nul(T) := \left\{ \vec{x} | \vec{x} \in V , T(\vec{x}) = \vec{0} \right\} = T^{-1}(\vec{0})\]

  It is a theorem that $Nul(T)$ is a vector subspace of $V$. The null space is sometimes called the kernel of $T$.
\end{defn}

\begin{defn}
  Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, we define the \textbf{column space} as
  \[Col(A) := span(A) = \left\{\mathbf{A}\vec{x} | \forall \vec{x} \in \mathbb{R}^{n}\right\}=  \left\{\vec{b} | \quad  \exists \vec{x} \in \mathbb{R}^{m \times n}, \mathbf{A}\vec{x} = \vec{b}\right \}\]
  The column space is also called the \textbf{range} of the linear transformation $\vec{x} \mapsto \mathbf{A} \vec{x}$.

  It is a theorem that $Col(\mathbf{A})$ is a vector subspace of $\mathbb{R}^{m}$.
\end{defn}

\begin{ex}
  Let \[A := \begin{bmatrix} 1 & 1 & 1  \\ 1 & 2 & -1 \\ 2 & 4 & -2 \end{bmatrix}\]
  Find the column space and the null space.
  \[Col(A) = span(\{\begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix} , \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix}\})\]
  \[Nul(A) = span(\{\begin{bmatrix} -3 \\ 2 \\ 1 \end{bmatrix}\})\]
\end{ex}
\section{April 27}

\begin{ex}
  Consider $T : \mathbb{P}_{2}  \to \mathbb{R}^{2 \times 3}$
  \[T(p(t) = \begin{bmatrix} p(0) & p(0) & p(1) \\ p(1) & p(1) & p(0) \end{bmatrix}\]

  Prove that $T$ is a linear transformation. \\
  $T$ satisfies the linear transformation axioms. $T$ preserves vector addition and preserves scalar multiplication.

  Find $Ker(T)$.

  \[Ker(T) = span \{ \lambda x. x^{2} - x \}\]

  Is $T$ a matrix transformation? According to YHPL, no.

  Find $Range(T)$.
  \[Range(T) = span \left\{ \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}, \begin{bmatrix} 0 & 0 & 1 \\ 1 & -1 & 0 \end{bmatrix}, \right\}\]

  Find all pre-images of $\begin{bmatrix} 2 & 2 & 1 \\ 1 & 1 & 2 \end{bmatrix}$?
  \[T^{-1}(\begin{bmatrix} 2 & 2 & 1 \\ 1 & 1 & 2 \end{bmatrix}) = \left\{ \lambda x. ax^{2}- (1 + a) x + 2 | \forall a \in \mathbb{R}\right\}\]


  Is $T$ injective/one-to-one? No because nullspace is not trivial.


  Is $T$ surjective/onto? Impossible because the codomain is larger than the domain.

\end{ex}
\subsection{4.3 -- Basis}

\begin{defn}
  Let $V$ be a vector space. We say that $B \subseteq V$ is called a \textbf{basis} of $V$ if and only if
  \begin{enumerate}
    \item $span(B) = V$
    \item $B$ is linearly independent
  \end{enumerate}
  In other words, a basis is a smallest spanning set.
\end{defn}

Note that a basis of $V$ is not unique: $V$ usually has infinitely many different bases.
For example, $\{1,x,x^{2}\}$ is a basis of $\mathbb{P}_{2}$, but $\{1 + x, x^{2}, x + x^{2}\}$ is also a basis of $\mathbb{P}_{2}$.
\begin{ex}
  Here are some examples of a basis
  \begin{enumerate}
    \item $\left\{\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\right\}$ is a basis of $\mathbb{R}^{3}$
    \item $\{1, x, x^{2}\}$ is a basis of $\mathbb{P}_{2}$

  \end{enumerate}
\end{ex}

\begin{thm}
  If $S$ spans $V$, then any superset of $S$ also spans $V$.

  If $S$ is a linearly independent set, then any subset of $S$ is also linearly indepenent.

  If $S$ is a linearly dependent set, then any superset of $S$ is also linearly depenent.

  If $B$ is a basis of $V$, then you cannot add or remove any vector from $B$ and still have it be a basis of $V$.
\end{thm}
\end{document}
